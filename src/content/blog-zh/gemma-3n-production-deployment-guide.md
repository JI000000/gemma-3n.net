---
title: "Gemma 3n ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æŒ‡å—ï¼š2025å¹´æœ€ä½³å®è·µ"
title_en: "Gemma 3n Production Deployment Guide: Best Practices for 2025"
description: "å­¦ä¹ å¦‚ä½•åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²Gemma 3næ¨¡å‹ï¼Œæ¶µç›–Dockerã€Kubernetesã€äº‘å¹³å°éƒ¨ç½²å’Œæ€§èƒ½ä¼˜åŒ–ã€‚"
description_en: "Learn how to deploy Gemma 3n models in production environments with Docker, Kubernetes, cloud platforms, and performance optimization."
pubDate: 2025-08-12
lastUpdated: 2025-08-12
author: "Gemma-3n.net Team"
tags: ["gemma-3n", "éƒ¨ç½²", "ç”Ÿäº§ç¯å¢ƒ", "docker", "kubernetes", "äº‘å¹³å°"]
draft: false
lang: "zh"
---

# Gemma 3n ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æŒ‡å—ï¼š2025å¹´æœ€ä½³å®è·µ

*æœ€åæ›´æ–°ï¼š2025å¹´8æœˆ12æ—¥*

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²AIæ¨¡å‹éœ€è¦ä»”ç»†è§„åˆ’ã€å¼ºå¤§çš„åŸºç¡€è®¾æ–½å’ŒæŒç»­ç›‘æ§ã€‚æœ¬æŒ‡å—å°†å¸¦æ‚¨å®ŒæˆGemma 3næ¨¡å‹åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„éƒ¨ç½²ã€‚

## ğŸ—ï¸ åŸºç¡€è®¾æ–½è§„åˆ’

### ç³»ç»Ÿè¦æ±‚

#### ç¡¬ä»¶è¦æ±‚
- **CPU**ï¼š8+æ ¸å¿ƒï¼ˆé«˜ååé‡å»ºè®®16+æ ¸å¿ƒï¼‰
- **å†…å­˜**ï¼šæœ€å°‘16GBï¼ˆE4Bæ¨¡å‹å»ºè®®32GB+ï¼‰
- **GPU**ï¼šNVIDIA RTX 4090æˆ–æ›´å¥½ï¼ˆç”¨äºGPUåŠ é€Ÿï¼‰
- **å­˜å‚¨**ï¼š50GB+ SSDå­˜å‚¨
- **ç½‘ç»œ**ï¼š100Mbps+å¸¦å®½

#### è½¯ä»¶è¦æ±‚
- **æ“ä½œç³»ç»Ÿ**ï¼šUbuntu 20.04+æˆ–CentOS 8+
- **Python**ï¼š3.9+
- **CUDA**ï¼š11.8+ï¼ˆç”¨äºGPUæ”¯æŒï¼‰
- **Docker**ï¼š20.10+
- **Kubernetes**ï¼š1.24+ï¼ˆç”¨äºå®¹å™¨ç¼–æ’ï¼‰

## ğŸ³ Dockeréƒ¨ç½²

### åˆ›å»ºç”Ÿäº§Dockerfile

```dockerfile
FROM python:3.11-slim

RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

EXPOSE 8000

HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Requirementsæ–‡ä»¶

```txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
transformers==4.35.0
torch==2.1.0
accelerate==0.24.1
sentencepiece==0.1.99
protobuf==4.25.1
pydantic==2.5.0
redis==5.0.1
prometheus-client==0.19.0
structlog==23.2.0
```

### FastAPIåº”ç”¨ç¤ºä¾‹

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import os

app = FastAPI(title="Gemma 3n API", version="1.0.0")

MODEL_NAME = os.getenv("MODEL_NAME", "google/gemma-3n-4b-it")

# åŠ è½½æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_4bit=True
)

class GenerationRequest(BaseModel):
    prompt: str
    max_length: int = 512
    temperature: float = 0.7

class GenerationResponse(BaseModel):
    text: str
    tokens_used: int

@app.get("/health")
async def health_check():
    return {"status": "healthy", "model": MODEL_NAME}

@app.post("/generate", response_model=GenerationResponse)
async def generate_text(request: GenerationRequest):
    try:
        inputs = tokenizer(request.prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                inputs.input_ids,
                max_length=request.max_length,
                temperature=request.temperature,
                pad_token_id=tokenizer.eos_token_id
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        tokens_used = len(outputs[0])
        
        return GenerationResponse(
            text=generated_text,
            tokens_used=tokens_used
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

## â˜ï¸ äº‘å¹³å°éƒ¨ç½²

### AWS ECSéƒ¨ç½²

```yaml
# task-definition.json
{
  "family": "gemma-api",
  "networkMode": "awsvpc",
  "requiresCompatibilities": ["FARGATE"],
  "cpu": "4096",
  "memory": "32768",
  "containerDefinitions": [
    {
      "name": "gemma-api",
      "image": "your-ecr-repo/gemma-api:latest",
      "portMappings": [
        {
          "containerPort": 8000,
          "protocol": "tcp"
        }
      ],
      "environment": [
        {
          "name": "MODEL_NAME",
          "value": "google/gemma-3n-4b-it"
        }
      ]
    }
  ]
}
```

### Kuberneteséƒ¨ç½²

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gemma-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: gemma-api
  template:
    metadata:
      labels:
        app: gemma-api
    spec:
      containers:
      - name: gemma-api
        image: gemma-api:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            memory: "16Gi"
            cpu: "4"
          limits:
            memory: "32Gi"
            cpu: "8"
        env:
        - name: MODEL_NAME
          value: "google/gemma-3n-4b-it"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 30
---
apiVersion: v1
kind: Service
metadata:
  name: gemma-api-service
spec:
  selector:
    app: gemma-api
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
```

## ğŸ“Š ç›‘æ§å’Œå¯è§‚æµ‹æ€§

### PrometheusæŒ‡æ ‡

```python
from prometheus_client import Counter, Histogram, Gauge

REQUEST_COUNT = Counter('gemma_requests_total', 'Total requests', ['model', 'status'])
REQUEST_DURATION = Histogram('gemma_request_duration_seconds', 'Request duration', ['model'])
MODEL_MEMORY_USAGE = Gauge('gemma_model_memory_bytes', 'Memory usage of model')
```

### Grafanaä»ªè¡¨æ¿é…ç½®

```json
{
  "dashboard": {
    "title": "Gemma 3n ç”Ÿäº§ç¯å¢ƒæŒ‡æ ‡",
    "panels": [
      {
        "title": "è¯·æ±‚é€Ÿç‡",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(gemma_requests_total[5m])",
            "legendFormat": "{{status}}"
          }
        ]
      },
      {
        "title": "å“åº”æ—¶é—´",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(gemma_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          }
        ]
      }
    ]
  }
}
```

## ğŸ”’ å®‰å…¨æœ€ä½³å®è·µ

### ç½‘ç»œå®‰å…¨ç­–ç•¥

```yaml
# network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: gemma-api-network-policy
spec:
  podSelector:
    matchLabels:
      app: gemma-api
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: frontend
    ports:
    - protocol: TCP
      port: 8000
```

### å¯†é’¥ç®¡ç†

```yaml
# secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: gemma-api-secrets
type: Opaque
data:
  api-key: <base64-encoded-api-key>
  model-token: <base64-encoded-model-token>
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

### æ¨¡å‹ä¼˜åŒ–

```python
import torch
from transformers import AutoModelForCausalLM

def optimize_model():
    model_name = "google/gemma-3n-4b-it"
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4"
    )
    
    model.eval()
    return model
```

### ç¼“å­˜ç­–ç•¥

```python
import redis
import hashlib
import json

class ModelCache:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.default_ttl = 3600
    
    def get_cache_key(self, prompt, params):
        data = {
            'prompt': prompt,
            'params': params
        }
        return f"gemma:{hashlib.md5(json.dumps(data, sort_keys=True).encode()).hexdigest()}"
    
    def get(self, prompt, params):
        key = self.get_cache_key(prompt, params)
        result = self.redis.get(key)
        return json.loads(result) if result else None
    
    def set(self, prompt, params, result, ttl=None):
        key = self.get_cache_key(prompt, params)
        ttl = ttl or self.default_ttl
        self.redis.setex(key, ttl, json.dumps(result))
```

## ğŸ“ˆ æ‰©å±•ç­–ç•¥

### æ°´å¹³Podè‡ªåŠ¨æ‰©å±•å™¨

```yaml
# hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: gemma-api-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: gemma-api
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

## ğŸ”§ CI/CDæµæ°´çº¿

### GitHub Actionså·¥ä½œæµ

```yaml
name: éƒ¨ç½²Gemma API

on:
  push:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: è®¾ç½®Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    - name: å®‰è£…ä¾èµ–
      run: |
        pip install -r requirements.txt
        pip install pytest
    - name: è¿è¡Œæµ‹è¯•
      run: pytest tests/

  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: æ„å»ºDockeré•œåƒ
      run: |
        docker build -t gemma-api:latest .
        docker tag gemma-api:latest your-registry/gemma-api:latest
        docker push your-registry/gemma-api:latest

  deploy:
    needs: build
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: éƒ¨ç½²åˆ°Kubernetes
      run: |
        kubectl set image deployment/gemma-api gemma-api=your-registry/gemma-api:latest
        kubectl rollout status deployment/gemma-api
```

## ğŸ“Š æˆæœ¬ä¼˜åŒ–

### èµ„æºç›‘æ§

```python
import psutil
import time

class ResourceOptimizer:
    def __init__(self):
        self.memory_threshold = 0.8
        self.cpu_threshold = 0.7
    
    def monitor_resources(self):
        memory_percent = psutil.virtual_memory().percent / 100
        cpu_percent = psutil.cpu_percent() / 100
        
        if memory_percent > self.memory_threshold:
            self.handle_high_memory()
        
        if cpu_percent > self.cpu_threshold:
            self.handle_high_cpu()
    
    def handle_high_memory(self):
        # å®æ–½å†…å­˜ä¼˜åŒ–ç­–ç•¥
        pass
    
    def handle_high_cpu(self):
        # å®æ–½CPUä¼˜åŒ–ç­–ç•¥
        pass
```

## ğŸš¨ ç¾éš¾æ¢å¤

### å¤‡ä»½ç­–ç•¥

```python
import boto3
import json
from datetime import datetime

class ModelBackup:
    def __init__(self, s3_bucket):
        self.s3 = boto3.client('s3')
        self.bucket = s3_bucket
    
    def backup_model_config(self, model_path):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_key = f"backups/model_config_{timestamp}.json"
        
        config = {
            "model_path": model_path,
            "timestamp": timestamp,
            "version": "1.0.0"
        }
        
        self.s3.put_object(
            Bucket=self.bucket,
            Key=backup_key,
            Body=json.dumps(config)
        )
        
        return backup_key
```

## ğŸ“ ç»“è®º

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²Gemma 3néœ€è¦ä»”ç»†è€ƒè™‘åŸºç¡€è®¾æ–½ã€å®‰å…¨æ€§ã€ç›‘æ§å’Œä¼˜åŒ–ã€‚æœ¬æŒ‡å—æä¾›äº†æˆåŠŸéƒ¨ç½²çš„åŸºæœ¬æ¡†æ¶ï¼Œä½†è¯·è®°ä½ï¼š

1. **ä»å°å¼€å§‹**ï¼šä»å•å®ä¾‹å¼€å§‹ï¼Œé€æ­¥æ‰©å±•
2. **å…¨é¢ç›‘æ§**ï¼šä»ç¬¬ä¸€å¤©å¼€å§‹å®æ–½å…¨é¢ç›‘æ§
3. **æ•…éšœè§„åˆ’**ï¼šè®¾è®¡ç³»ç»Ÿä»¥ä¼˜é›…åœ°å¤„ç†æ•…éšœ
4. **æŒç»­ä¼˜åŒ–**ï¼šå®šæœŸå®¡æŸ¥å’Œä¼˜åŒ–æ€§èƒ½
5. **ä¿æŒæ›´æ–°**ï¼šä½¿æ‚¨çš„éƒ¨ç½²ä¸æœ€æ–°çš„Gemma 3næ›´æ–°ä¿æŒä¸€è‡´

é€šè¿‡éµå¾ªè¿™äº›æœ€ä½³å®è·µï¼Œæ‚¨å¯ä»¥æ„å»ºä¸€ä¸ªå¼ºå¤§ã€å¯æ‰©å±•ä¸”å…·æœ‰æˆæœ¬æ•ˆç›Šçš„Gemma 3nç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ï¼Œä¸ºç”¨æˆ·æä¾›å¯é çš„æœåŠ¡ã€‚

---

*æœ‰å…³æ›´å¤šéƒ¨ç½²èµ„æºå’Œç¤¾åŒºæ”¯æŒï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„[GitHubä»“åº“](https://github.com/google/gemma)å’Œ[Discordç¤¾åŒº](https://discord.gg/gemma)ã€‚*
