---
title: "Gemma 3n è§†é¢‘è¾“å…¥å¤„ç†å®Œæ•´æŒ‡å— (2025)"
title_en: "Complete Guide to Gemma 3n Video Input Processing (2025)"
description: "æŒæ¡ Gemma 3n çš„è§†é¢‘è¾“å…¥å¤„ç†ã€‚å­¦ä¹ å¦‚ä½•ä½¿ç”¨å¤šæ¨¡æ€ AI åŠŸèƒ½åˆ†æã€ç†è§£å’Œç”Ÿæˆè§†é¢‘å†…å®¹çš„å“åº”ã€‚"
description_en: "Master video input processing with Gemma 3n. Learn how to analyze, understand, and generate responses from video content using multimodal AI capabilities."
pubDate: 2025-08-12
lang: "zh"
author: "Gemma-3n.net Team"
image: "/images/gemma-3n-video-input.jpg"
tags: ["gemma 3n", "video input", "multimodal", "ai models", "video analysis", "computer vision"]
---

# Gemma 3n è§†é¢‘è¾“å…¥å¤„ç†å®Œæ•´æŒ‡å— (2025)

> æŒæ¡ Gemma 3n å…ˆè¿›å¤šæ¨¡æ€èƒ½åŠ›çš„è§†é¢‘è¾“å…¥å¤„ç†æŠ€æœ¯

## ğŸ¯ ç†è§£ Gemma 3n ä¸­çš„è§†é¢‘è¾“å…¥

Gemma 3n çš„è§†é¢‘è¾“å…¥åŠŸèƒ½ä»£è¡¨äº†å¤šæ¨¡æ€ AI çš„é‡å¤§è¿›æ­¥ã€‚è¯¥æ¨¡å‹å¯ä»¥å¤„ç†è§†é¢‘å†…å®¹ï¼Œæå–æ—¶é—´å’Œç©ºé—´ä¿¡æ¯ï¼Œå¹¶åŸºäºè§†è§‰ç†è§£ç”Ÿæˆä¸Šä¸‹æ–‡å“åº”ã€‚

### ä¸»è¦ç‰¹æ€§
- **æ—¶é—´ç†è§£**: åˆ†æè§†é¢‘åºåˆ—çš„æ—¶é—´å˜åŒ–
- **ç©ºé—´è¯†åˆ«**: è¯†åˆ«ç‰©ä½“ã€åœºæ™¯å’ŒåŠ¨ä½œ
- **ä¸Šä¸‹æ–‡åˆ†æ**: ç†è§£è§†è§‰å…ƒç´ ä¹‹é—´çš„å…³ç³»
- **è‡ªç„¶è¯­è¨€ç”Ÿæˆ**: ç”Ÿæˆæè¿°æ€§å’Œåˆ†ææ€§å“åº”

## ğŸ“¦ ç¯å¢ƒå‡†å¤‡å’Œè®¾ç½®

### å¿…éœ€çš„ä¾èµ–
```bash
pip install torch torchvision transformers accelerate
pip install opencv-python moviepy pillow
```

### æ¨¡å‹è®¾ç½®
```python
from transformers import AutoProcessor, AutoModelForCausalLM
import torch

# åŠ è½½ Gemma 3n 4B æ¨¡å‹ï¼ˆæ”¯æŒè§†é¢‘è¾“å…¥ï¼‰
model_name = "google/gemma-3n-4b"
processor = AutoProcessor.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)
```

## ğŸ¬ åŸºç¡€è§†é¢‘è¾“å…¥å¤„ç†

### 1. åŠ è½½å’Œé¢„å¤„ç†è§†é¢‘

```python
import cv2
import numpy as np
from PIL import Image

def load_video_frames(video_path, max_frames=16):
    """ä»è§†é¢‘ä¸­æå–å¸§è¿›è¡Œå¤„ç†"""
    cap = cv2.VideoCapture(video_path)
    frames = []
    
    frame_count = 0
    while cap.isOpened() and frame_count < max_frames:
        ret, frame = cap.read()
        if not ret:
            break
            
        # è½¬æ¢ BGR åˆ° RGB å¹¶è°ƒæ•´å¤§å°
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frame_resized = cv2.resize(frame_rgb, (224, 224))
        frame_pil = Image.fromarray(frame_resized)
        frames.append(frame_pil)
        
        frame_count += 1
    
    cap.release()
    return frames

# ä½¿ç”¨ç¤ºä¾‹
video_frames = load_video_frames("sample_video.mp4", max_frames=16)
```

### 2. ä½¿ç”¨ Gemma 3n åˆ†æè§†é¢‘

```python
def analyze_video(video_frames, prompt="æè¿°ä½ åœ¨è¿™ä¸ªè§†é¢‘ä¸­çœ‹åˆ°çš„å†…å®¹ï¼š"):
    """ä½¿ç”¨ Gemma 3n åˆ†æè§†é¢‘å†…å®¹"""
    
    # å‡†å¤‡è¾“å…¥
    inputs = processor(
        text=prompt,
        images=video_frames,
        return_tensors="pt"
    )
    
    # ç”Ÿæˆå“åº”
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=200,
            temperature=0.7,
            do_sample=True
        )
    
    # è§£ç å“åº”
    response = processor.decode(outputs[0], skip_special_tokens=True)
    return response

# ä½¿ç”¨ç¤ºä¾‹
description = analyze_video(video_frames, "è¿™ä¸ªè§†é¢‘ä¸­å‘ç”Ÿäº†ä»€ä¹ˆåŠ¨ä½œï¼Ÿ")
print(description)
```

## ğŸ”§ é«˜çº§è§†é¢‘å¤„ç†æŠ€æœ¯

### 1. å¸§é‡‡æ ·ç­–ç•¥

```python
def extract_key_frames(video_path, sampling_rate=0.1):
    """ä½¿ç”¨å‡åŒ€é‡‡æ ·æå–å…³é”®å¸§"""
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    # è®¡ç®—è¦æå–çš„å¸§ç´¢å¼•
    frame_indices = np.linspace(0, total_frames-1, 
                               int(total_frames * sampling_rate), 
                               dtype=int)
    
    frames = []
    for idx in frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if ret:
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame_resized = cv2.resize(frame_rgb, (224, 224))
            frame_pil = Image.fromarray(frame_resized)
            frames.append(frame_pil)
    
    cap.release()
    return frames
```

### 2. æ—¶é—´åˆ†æ

```python
def analyze_video_temporal(video_frames):
    """ä½¿ç”¨æ—¶é—´ç†è§£åˆ†æè§†é¢‘"""
    temporal_prompts = [
        "è¿™ä¸ªè§†é¢‘å¼€å§‹æ—¶å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ",
        "è§†é¢‘ä¸­é—´å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ",
        "è¿™ä¸ªè§†é¢‘æ˜¯å¦‚ä½•ç»“æŸçš„ï¼Ÿ"
    ]
    
    results = []
    for prompt in temporal_prompts:
        inputs = processor(
            text=prompt,
            images=video_frames,
            return_tensors="pt"
        )
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=150,
                temperature=0.7
            )
        
        response = processor.decode(outputs[0], skip_special_tokens=True)
        results.append(response)
    
    return results
```

## ğŸ¨ ä¸“ä¸šè§†é¢‘åˆ†æä»»åŠ¡

### 1. åŠ¨ä½œè¯†åˆ«

```python
def recognize_actions(video_frames):
    """è¯†åˆ«å’Œæè¿°è§†é¢‘ä¸­çš„åŠ¨ä½œ"""
    action_prompts = [
        "è¿™ä¸ªè§†é¢‘ä¸­æ­£åœ¨æ‰§è¡Œä»€ä¹ˆåŠ¨ä½œï¼Ÿ",
        "æè¿°ä½ è§‚å¯Ÿåˆ°çš„è¿åŠ¨å’Œæ´»åŠ¨ï¼š",
        "è¿™é‡Œå‘ç”Ÿçš„ä¸»è¦åŠ¨ä½œæ˜¯ä»€ä¹ˆï¼Ÿ"
    ]
    
    results = []
    for prompt in action_prompts:
        inputs = processor(
            text=prompt,
            images=video_frames,
            return_tensors="pt"
        )
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=200,
                temperature=0.6
            )
        
        response = processor.decode(outputs[0], skip_special_tokens=True)
        results.append(response)
    
    return results
```

### 2. ç‰©ä½“å’Œåœºæ™¯åˆ†æ

```python
def analyze_objects_and_scenes(video_frames):
    """åˆ†æè§†é¢‘ä¸­çš„ç‰©ä½“å’Œåœºæ™¯"""
    
    analysis_prompts = [
        "ä½ èƒ½åœ¨è¿™ä¸ªè§†é¢‘ä¸­è¯†åˆ«å‡ºä»€ä¹ˆç‰©ä½“ï¼Ÿ",
        "æè¿°åœºæ™¯å’Œç¯å¢ƒï¼š",
        "è¿™ä¸ªè§†é¢‘çš„èƒŒæ™¯æˆ–ä½ç½®æ˜¯ä»€ä¹ˆï¼Ÿ",
        "æ˜¯å¦æœ‰å¯è§çš„äººæˆ–åŠ¨ç‰©ï¼Ÿ"
    ]
    
    results = {}
    for prompt in analysis_prompts:
        inputs = processor(
            text=prompt,
            images=video_frames,
            return_tensors="pt"
        )
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=150,
                temperature=0.7
            )
        
        response = processor.decode(outputs[0], skip_special_tokens=True)
        results[prompt] = response
    
    return results
```

## ğŸš€ å®æ—¶è§†é¢‘å¤„ç†

### æµå¼è§†é¢‘åˆ†æ

```python
import threading
import queue
import time

class RealTimeVideoAnalyzer:
    def __init__(self, model, processor, frame_buffer_size=8):
        self.model = model
        self.processor = processor
        self.frame_buffer = queue.Queue(maxsize=frame_buffer_size)
        self.analysis_results = queue.Queue()
        self.running = False
    
    def start_analysis(self, video_source=0):
        """å¼€å§‹å®æ—¶è§†é¢‘åˆ†æ"""
        self.running = True
        
        # å¯åŠ¨è§†é¢‘æ•è·çº¿ç¨‹
        capture_thread = threading.Thread(
            target=self._capture_frames, 
            args=(video_source,)
        )
        capture_thread.start()
        
        # å¯åŠ¨åˆ†æçº¿ç¨‹
        analysis_thread = threading.Thread(
            target=self._analyze_frames
        )
        analysis_thread.start()
        
        return capture_thread, analysis_thread
    
    def _capture_frames(self, video_source):
        """ä»è§†é¢‘æºæ•è·å¸§"""
        cap = cv2.VideoCapture(video_source)
        
        while self.running:
            ret, frame = cap.read()
            if not ret:
                break
            
            # å¤„ç†å¸§
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame_resized = cv2.resize(frame_rgb, (224, 224))
            frame_pil = Image.fromarray(frame_resized)
            
            # æ·»åŠ åˆ°ç¼“å†²åŒº
            if not self.frame_buffer.full():
                self.frame_buffer.put(frame_pil)
            else:
                # ç§»é™¤æœ€æ—§çš„å¸§
                try:
                    self.frame_buffer.get_nowait()
                    self.frame_buffer.put(frame_pil)
                except queue.Empty:
                    pass
            
            time.sleep(0.1)  # 10 FPS
        
        cap.release()
    
    def _analyze_frames(self):
        """åˆ†æç¼“å†²åŒºä¸­çš„å¸§"""
        while self.running:
            if self.frame_buffer.qsize() >= 4:
                # è·å–ç”¨äºåˆ†æçš„å¸§
                frames = []
                for _ in range(4):
                    try:
                        frame = self.frame_buffer.get_nowait()
                        frames.append(frame)
                    except queue.Empty:
                        break
                
                if len(frames) >= 4:
                    # åˆ†æå¸§
                    inputs = self.processor(
                        text="è¿™ä¸ªè§†é¢‘æµä¸­æ­£åœ¨å‘ç”Ÿä»€ä¹ˆï¼Ÿ",
                        images=frames,
                        return_tensors="pt"
                    )
                    
                    with torch.no_grad():
                        outputs = self.model.generate(
                            **inputs,
                            max_length=100,
                            temperature=0.7
                        )
                    
                    response = self.processor.decode(
                        outputs[0], 
                        skip_special_tokens=True
                    )
                    
                    # å­˜å‚¨ç»“æœ
                    self.analysis_results.put(response)
            
            time.sleep(1.0)  # æ¯ç§’åˆ†æä¸€æ¬¡
    
    def get_latest_analysis(self):
        """è·å–æœ€æ–°çš„åˆ†æç»“æœ"""
        try:
            return self.analysis_results.get_nowait()
        except queue.Empty:
            return None
    
    def stop(self):
        """åœæ­¢åˆ†æ"""
        self.running = False

# ä½¿ç”¨ç¤ºä¾‹
analyzer = RealTimeVideoAnalyzer(model, processor)
capture_thread, analysis_thread = analyzer.start_analysis()

# ç›‘æ§ç»“æœ
for _ in range(10):
    result = analyzer.get_latest_analysis()
    if result:
        print(f"å®æ—¶åˆ†æ: {result}")
    time.sleep(2)

analyzer.stop()
```

## ğŸ” æ€§èƒ½ä¼˜åŒ–

### å†…å­˜ç®¡ç†

```python
def optimize_video_processing(video_frames, batch_size=4):
    """åˆ†æ‰¹å¤„ç†è§†é¢‘å¸§ä»¥æé«˜å†…å­˜æ•ˆç‡"""
    results = []
    
    for i in range(0, len(video_frames), batch_size):
        batch_frames = video_frames[i:i + batch_size]
        
        # å¤„ç†æ‰¹æ¬¡
        inputs = processor(
            text="åˆ†æè¿™ä¸ªè§†é¢‘ç‰‡æ®µï¼š",
            images=batch_frames,
            return_tensors="pt"
        )
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=150,
                temperature=0.7
            )
        
        response = processor.decode(outputs[0], skip_special_tokens=True)
        results.append(response)
        
        # æ¸…ç†å†…å­˜
        del inputs, outputs
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
    
    return results
```

## ğŸ› ï¸ å¸¸è§é—®é¢˜æ•…éšœæ’é™¤

### 1. å†…å­˜é—®é¢˜
```python
# ä¸ºå†…å­˜å—é™çš„ç³»ç»Ÿå‡å°‘å¸§åˆ†è¾¨ç‡
def load_video_frames_low_memory(video_path, max_frames=8, size=(112, 112)):
    """ä»¥å‡å°‘çš„å†…å­˜ä½¿ç”¨é‡åŠ è½½è§†é¢‘å¸§"""
    cap = cv2.VideoCapture(video_path)
    frames = []
    
    frame_count = 0
    while cap.isOpened() and frame_count < max_frames:
        ret, frame = cap.read()
        if not ret:
            break
        
        # ä½¿ç”¨è¾ƒå°çš„å¸§å¤§å°
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frame_resized = cv2.resize(frame_rgb, size)
        frame_pil = Image.fromarray(frame_resized)
        frames.append(frame_pil)
        
        frame_count += 1
    
    cap.release()
    return frames
```

### 2. å¤„ç†é€Ÿåº¦é—®é¢˜
```python
# ä½¿ç”¨å¸§è·³è¿‡è¿›è¡Œæ›´å¿«çš„å¤„ç†
def load_video_frames_fast(video_path, skip_frames=2, max_frames=16):
    """ä½¿ç”¨å¸§è·³è¿‡æ›´å¿«åœ°åŠ è½½è§†é¢‘å¸§"""
    cap = cv2.VideoCapture(video_path)
    frames = []
    
    frame_count = 0
    while cap.isOpened() and frame_count < max_frames:
        ret, frame = cap.read()
        if not ret:
            break
        
        # è·³è¿‡å¸§ä»¥è¿›è¡Œæ›´å¿«çš„å¤„ç†
        if frame_count % (skip_frames + 1) == 0:
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame_resized = cv2.resize(frame_rgb, (224, 224))
            frame_pil = Image.fromarray(frame_resized)
            frames.append(frame_pil)
        
        frame_count += 1
    
    cap.release()
    return frames
```

## ğŸ“Š æœ€ä½³å®è·µ

### 1. è§†é¢‘è´¨é‡è€ƒè™‘
- **åˆ†è¾¨ç‡**: ä½¿ç”¨ 224x224 æˆ– 112x112 è·å¾—æœ€ä½³æ€§èƒ½
- **å¸§ç‡**: ä¸ºå¤§å¤šæ•°åˆ†ææå– 8-16 å¸§
- **æ ¼å¼**: MP4 æˆ– AVI æ ¼å¼æ•ˆæœæœ€ä½³
- **å‹ç¼©**: å¹³è¡¡è´¨é‡ä¸æ–‡ä»¶å¤§å°

### 2. åˆ†æç­–ç•¥
- **å…³é”®å¸§æå–**: ä½¿ç”¨åœºæ™¯å˜åŒ–æ£€æµ‹è·å–ç›¸å…³å¸§
- **æ—¶é—´é‡‡æ ·**: å‡åŒ€é‡‡æ ·é€‚ç”¨äºå¤§å¤šæ•°æƒ…å†µ
- **æ‰¹å¤„ç†**: åˆ†æ‰¹å¤„ç†å¸§ä»¥æé«˜å†…å­˜æ•ˆç‡

### 3. æ€§èƒ½ä¼˜åŒ–
- **GPU åŠ é€Ÿ**: åœ¨å¯ç”¨æ—¶ä½¿ç”¨ CUDA
- **å†…å­˜ç®¡ç†**: åœ¨æ‰¹æ¬¡ä¹‹é—´æ¸…ç†ç¼“å­˜
- **å¸§è·³è¿‡**: è·³è¿‡å¸§ä»¥è¿›è¡Œæ›´å¿«çš„å¤„ç†
- **åˆ†è¾¨ç‡é™ä½**: å¯¹å®æ—¶å¤„ç†ä½¿ç”¨è¾ƒä½åˆ†è¾¨ç‡

## ğŸš€ ä¸‹ä¸€æ­¥

1. **å°è¯•ä¸åŒçš„è§†é¢‘ç±»å‹**: ä½¿ç”¨å„ç§è§†é¢‘å†…å®¹è¿›è¡Œæµ‹è¯•
2. **ä¸ºæ‚¨çš„ç”¨ä¾‹ä¼˜åŒ–**: ä¸ºæ‚¨çš„ç‰¹å®šéœ€æ±‚è°ƒæ•´å‚æ•°
3. **å®ç°å®æ—¶å¤„ç†**: è®¾ç½®æµå¼è§†é¢‘åˆ†æ
4. **æ„å»ºè§†é¢‘åˆ†æåº”ç”¨ç¨‹åº**: åˆ›å»ºè‡ªå®šä¹‰è§†é¢‘åˆ†æå·¥å…·

## ğŸ“š å…¶ä»–èµ„æº

- [Gemma 3n æ¨¡å‹æ–‡æ¡£](https://huggingface.co/google/gemma-3n-4b)
- [OpenCV è§†é¢‘å¤„ç†æŒ‡å—](https://docs.opencv.org/)
- [Transformers å¤šæ¨¡æ€æ–‡æ¡£](https://huggingface.co/docs/transformers/multimodal)

---

**å‡†å¤‡ä½¿ç”¨ Gemma 3n å¤„ç†è§†é¢‘ï¼Ÿ** ä»åŸºç¡€è§†é¢‘åŠ è½½å¼€å§‹ï¼Œé€æ­¥æ¢ç´¢é«˜çº§åˆ†ææŠ€æœ¯ã€‚

**éœ€è¦å¸®åŠ©ï¼Ÿ** æŸ¥çœ‹æˆ‘ä»¬çš„[ç¤¾åŒºè®ºå›](https://gemma-3n.net/community)è·å–æ”¯æŒå’Œè®¨è®ºã€‚
