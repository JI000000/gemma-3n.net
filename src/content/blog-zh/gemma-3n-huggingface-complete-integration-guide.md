---
title: "Gemma 3n ä¸ Hugging Face å®Œæ•´é›†æˆæŒ‡å— (2025)"
title_en: "Complete Guide to Gemma 3n Integration with Hugging Face (2025)"
description: "å­¦ä¹ å¦‚ä½•å°† Gemma 3n ä¸ Hugging Face é›†æˆï¼Œç”¨äºæ¨¡å‹éƒ¨ç½²ã€æ¨ç†å’Œå¾®è°ƒã€‚åŒ…å«ä»£ç ç¤ºä¾‹çš„è¯¦ç»†æŒ‡å—ã€‚"
description_en: "Learn how to integrate Gemma 3n with Hugging Face for model deployment, inference, and fine-tuning. Step-by-step guide with code examples."
pubDate: 2025-08-12
lang: "zh"
author: "Gemma-3n.net Team"
image: "/images/gemma-3n-huggingface-integration.jpg"
tags: ["gemma 3n", "hugging face", "ai models", "deployment", "inference", "fine-tuning"]
---

# Gemma 3n ä¸ Hugging Face å®Œæ•´é›†æˆæŒ‡å— (2025)

> æŒæ¡ Gemma 3n ä¸ Hugging Face çš„é›†æˆï¼Œå®ç°æ— ç¼çš„ AI æ¨¡å‹éƒ¨ç½²å’Œæ¨ç†

## ğŸ¯ ä¸ºä»€ä¹ˆé€‰æ‹© Hugging Face + Gemma 3nï¼Ÿ

Hugging Face å·²æˆä¸º AI æ¨¡å‹éƒ¨ç½²å’Œåˆ†äº«çš„é¦–é€‰å¹³å°ã€‚å‡­å€Ÿ Gemma 3n å¼ºå¤§çš„å¤šæ¨¡æ€èƒ½åŠ›ï¼Œä¸ Hugging Face é›†æˆå°†ä¸ºå¼€å‘è€…å’Œç ”ç©¶äººå‘˜å¼€å¯æ— é™å¯èƒ½ã€‚

### ä¸»è¦ä¼˜åŠ¿
- **ä¾¿æ·æ¨¡å‹è®¿é—®**: ç›´æ¥ä¸‹è½½å’Œä½¿ç”¨ Gemma 3n æ¨¡å‹
- **äº‘ç«¯æ¨ç†**: æ— éœ€æœ¬åœ°ç¡¬ä»¶è¦æ±‚å³å¯è¿è¡Œæ¨¡å‹
- **å¾®è°ƒæ”¯æŒ**: åˆ©ç”¨ Hugging Face çš„è®­ç»ƒåŸºç¡€è®¾æ–½
- **ç¤¾åŒºåˆ†äº«**: ä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„å¾®è°ƒæ¨¡å‹
- **ç”Ÿäº§å°±ç»ª**: ä»¥æœ€å°‘çš„è®¾ç½®å°†æ¨¡å‹éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ

## ğŸ“¦ åœ¨ Hugging Face ä¸Šå¼€å§‹ä½¿ç”¨ Gemma 3n

### ç¯å¢ƒå‡†å¤‡
```bash
# å®‰è£…å¿…éœ€çš„åŒ…
pip install transformers torch accelerate
pip install huggingface_hub
```

### èº«ä»½éªŒè¯è®¾ç½®
```python
from huggingface_hub import login

# ç™»å½• Hugging Faceï¼ˆæ¨¡å‹è®¿é—®å¿…éœ€ï¼‰
login(token="your_hf_token")
```

## ğŸš€ æ¨¡å‹åŠ è½½å’Œæ¨ç†

### 1. åŸºç¡€æ–‡æœ¬ç”Ÿæˆ

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# åŠ è½½ Gemma 3n æ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "google/gemma-3n-2b"  # æˆ– "google/gemma-3n-4b"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# ç”Ÿæˆæ–‡æœ¬
prompt = "ç”¨ç®€å•çš„è¯è§£é‡Šé‡å­è®¡ç®—ï¼š"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(
    **inputs,
    max_length=200,
    temperature=0.7,
    do_sample=True
)

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

### 2. å¤šæ¨¡æ€æ¨ç†ï¼ˆE4B æ¨¡å‹ï¼‰

```python
from transformers import AutoProcessor, AutoModelForCausalLM
from PIL import Image

# åŠ è½½å¤šæ¨¡æ€æ¨¡å‹
model_name = "google/gemma-3n-4b"
processor = AutoProcessor.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# å¤„ç†å›¾åƒå’Œæ–‡æœ¬
image = Image.open("sample_image.jpg")
text = "æè¿°ä½ åœ¨è¿™å¼ å›¾ç‰‡ä¸­çœ‹åˆ°çš„å†…å®¹ï¼š"
inputs = processor(text=text, images=image, return_tensors="pt")

# ç”Ÿæˆå“åº”
outputs = model.generate(
    **inputs,
    max_length=150,
    temperature=0.8
)

response = processor.decode(outputs[0], skip_special_tokens=True)
print(response)
```

## ğŸ”§ é«˜çº§é…ç½®

### æ¨¡å‹é‡åŒ–ä»¥æé«˜æ•ˆç‡

```python
from transformers import BitsAndBytesConfig

# é…ç½® 4 ä½é‡åŒ–
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True
)

# åŠ è½½é‡åŒ–æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=quantization_config,
    device_map="auto"
)
```

### è‡ªå®šä¹‰ç”Ÿæˆå‚æ•°

```python
# é«˜çº§ç”Ÿæˆé…ç½®
generation_config = {
    "max_length": 512,
    "temperature": 0.7,
    "top_p": 0.9,
    "top_k": 50,
    "repetition_penalty": 1.1,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id
}

outputs = model.generate(**inputs, **generation_config)
```

## ğŸ¨ åœ¨ Hugging Face ä¸Šå¾®è°ƒ Gemma 3n

### 1. æ•°æ®é›†å‡†å¤‡

```python
from datasets import Dataset

# å‡†å¤‡æ‚¨çš„æ•°æ®é›†
data = {
    "text": [
        "é—®é¢˜ï¼šä»€ä¹ˆæ˜¯ AIï¼Ÿç­”æ¡ˆï¼šäººå·¥æ™ºèƒ½...",
        "é—®é¢˜ï¼šæœºå™¨å­¦ä¹ å¦‚ä½•å·¥ä½œï¼Ÿç­”æ¡ˆï¼šæœºå™¨å­¦ä¹ ...",
    ]
}
dataset = Dataset.from_dict(data)

# å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=512
    )

tokenized_dataset = dataset.map(tokenize_function, batched=True)
```

### 2. è®­ç»ƒé…ç½®

```python
from transformers import TrainingArguments, Trainer

# è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./gemma-3n-finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-5,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    load_best_model_at_end=True,
    push_to_hub=True,  # æ¨é€åˆ° Hugging Face Hub
    hub_model_id="your-username/gemma-3n-finetuned"
)

# åˆå§‹åŒ–è®­ç»ƒå™¨
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer
)

# å¼€å§‹è®­ç»ƒ
trainer.train()
```

### 3. æ¨¡å‹ä¸Šä¼ å’Œåˆ†äº«

```python
# æ¨é€æ¨¡å‹åˆ° Hugging Face Hub
trainer.push_to_hub()

# æˆ–æ‰‹åŠ¨ä¸Šä¼ 
model.push_to_hub("your-username/gemma-3n-custom")
tokenizer.push_to_hub("your-username/gemma-3n-custom")
```

## ğŸŒ éƒ¨ç½²é€‰é¡¹

### 1. Hugging Face æ¨ç† API

```python
import requests

# ä½¿ç”¨ Hugging Face æ¨ç† API
API_URL = "https://api-inference.huggingface.co/models/google/gemma-3n-2b"
headers = {"Authorization": f"Bearer {hf_token}"}

def query_model(payload):
    response = requests.post(API_URL, headers=headers, json=payload)
    return response.json()

# ä½¿ç”¨ç¤ºä¾‹
output = query_model({
    "inputs": "è§£é‡Šæœºå™¨å­¦ä¹ ï¼š",
    "parameters": {
        "max_length": 200,
        "temperature": 0.7
    }
})
print(output[0]["generated_text"])
```

### 2. Hugging Face Spaces éƒ¨ç½²

```python
# app.py ç”¨äº Hugging Face Spaces
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM

# åŠ è½½æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained("google/gemma-3n-2b")
model = AutoModelForCausalLM.from_pretrained("google/gemma-3n-2b")

def generate_text(prompt, max_length=200):
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(
        **inputs,
        max_length=max_length,
        temperature=0.7,
        do_sample=True
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# åˆ›å»º Gradio ç•Œé¢
iface = gr.Interface(
    fn=generate_text,
    inputs=[
        gr.Textbox(label="è¾“å…¥æç¤º"),
        gr.Slider(minimum=50, maximum=500, value=200, label="æœ€å¤§é•¿åº¦")
    ],
    outputs=gr.Textbox(label="ç”Ÿæˆçš„æ–‡æœ¬"),
    title="Gemma 3n æ–‡æœ¬ç”Ÿæˆå™¨"
)

iface.launch()
```

## ğŸ” æ€§èƒ½ä¼˜åŒ–

### å†…å­˜ç®¡ç†

```python
# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥æé«˜å†…å­˜æ•ˆç‡
model.gradient_checkpointing_enable()

# å¯¹å¤§æ¨¡å‹ä½¿ç”¨ CPU å¸è½½
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    offload_folder="offload",
    torch_dtype=torch.float16
)
```

### æ‰¹å¤„ç†

```python
# é«˜æ•ˆçš„æ‰¹å¤„ç†
def batch_generate(prompts, batch_size=4):
    results = []
    for i in range(0, len(prompts), batch_size):
        batch = prompts[i:i + batch_size]
        inputs = tokenizer(batch, return_tensors="pt", padding=True)
        outputs = model.generate(**inputs, max_length=200)
        batch_results = [tokenizer.decode(output, skip_special_tokens=True) 
                        for output in outputs]
        results.extend(batch_results)
    return results
```

## ğŸ› ï¸ å¸¸è§é—®é¢˜æ•…éšœæ’é™¤

### 1. å†…å­˜é—®é¢˜
```python
# é™ä½æ¨¡å‹ç²¾åº¦
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True
)

# ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
training_args = TrainingArguments(
    gradient_accumulation_steps=8,
    per_device_train_batch_size=1
)
```

### 2. åˆ†è¯é—®é¢˜
```python
# å¤„ç†ç¼ºå¤±çš„æ ‡è®°
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# è®¾ç½®å¡«å……æ ‡è®° ID
model.config.pad_token_id = tokenizer.pad_token_id
```

### 3. æ¨¡å‹åŠ è½½é—®é¢˜
```python
# å¼ºåˆ¶ä»ç¼“å­˜ä¸‹è½½
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    force_download=True,
    resume_download=True
)
```

## ğŸ“Š æœ€ä½³å®è·µ

### 1. æ¨¡å‹é€‰æ‹©
- **E2B (20äº¿å‚æ•°)**: é€‚åˆå¿«é€ŸåŸå‹è®¾è®¡å’Œæœ‰é™èµ„æº
- **E4B (40äº¿å‚æ•°)**: æ›´å¥½çš„æ€§èƒ½ï¼Œéœ€è¦æ›´å¤šèµ„æº
- **å¤šæ¨¡æ€**: é€‰æ‹© E4B è¿›è¡Œå›¾åƒ/éŸ³é¢‘å¤„ç†

### 2. èµ„æºç®¡ç†
- åœ¨ç”Ÿäº§éƒ¨ç½²ä¸­ä½¿ç”¨é‡åŒ–
- å®æ–½é€‚å½“çš„é”™è¯¯å¤„ç†
- ç›‘æ§å†…å­˜ä½¿ç”¨å’Œæ€§èƒ½

### 3. å®‰å…¨è€ƒè™‘
- åˆ‡å‹¿å°† API ä»¤ç‰Œæäº¤åˆ°ç‰ˆæœ¬æ§åˆ¶
- å¯¹æ•æ„Ÿæ•°æ®ä½¿ç”¨ç¯å¢ƒå˜é‡
- å¯¹ API è°ƒç”¨å®æ–½é€Ÿç‡é™åˆ¶

## ğŸš€ ä¸‹ä¸€æ­¥

1. **æ¢ç´¢æ¨¡å‹å˜ä½“**: å°è¯•ä¸åŒçš„ Gemma 3n é…ç½®
2. **ä¸ºæ‚¨çš„é¢†åŸŸå¾®è°ƒ**: ä½¿æ¨¡å‹é€‚åº”æ‚¨çš„ç‰¹å®šç”¨ä¾‹
3. **éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ**: ä½¿ç”¨ Hugging Face çš„éƒ¨ç½²é€‰é¡¹
4. **åŠ å…¥ç¤¾åŒº**: åˆ†äº«æ‚¨çš„æ¨¡å‹å¹¶å‘ä»–äººå­¦ä¹ 

## ğŸ“š å…¶ä»–èµ„æº

- [Hugging Face Gemma 3n æ¨¡å‹](https://huggingface.co/google/gemma-3n-2b)
- [Transformers æ–‡æ¡£](https://huggingface.co/docs/transformers)
- [Hugging Face Hub æŒ‡å—](https://huggingface.co/docs/hub)
- [æ¨¡å‹éƒ¨ç½²æœ€ä½³å®è·µ](https://huggingface.co/docs/hub/models-deploy)

---

**å‡†å¤‡å°† Gemma 3n ä¸ Hugging Face é›†æˆï¼Ÿ** ä»åŸºç¡€æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹å¼€å§‹ï¼Œé€æ­¥æ¢ç´¢å¤šæ¨¡æ€æ¨ç†å’Œå¾®è°ƒç­‰é«˜çº§åŠŸèƒ½ã€‚

**éœ€è¦å¸®åŠ©ï¼Ÿ** æŸ¥çœ‹æˆ‘ä»¬çš„[ç¤¾åŒºè®ºå›](https://gemma-3n.net/community)è·å–æ”¯æŒå’Œè®¨è®ºã€‚
