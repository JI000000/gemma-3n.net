// ÊîØÊåÅÁöÑËØ≠Ë®Ä
export const languages = {
  en: 'English',
  zh: '‰∏≠Êñá'
} as const;

export type Language = keyof typeof languages;

// ÈªòËÆ§ËØ≠Ë®Ä
export const defaultLang: Language = 'en';

// ËØ≠Ë®ÄÈÖçÁΩÆ
export const langConfig = {
  en: {
    name: 'English',
    dir: 'ltr',
    locale: 'en-US'
  },
  zh: {
    name: '‰∏≠Êñá',
    dir: 'ltr', 
    locale: 'zh-CN'
  }
} as const;

// UIÊñáÊú¨ËµÑÊ∫ê
export const ui = {
  en: {
    // Nav
    'nav.home': 'Home',
    'nav.about': 'About', 
    'nav.benchmarks': 'Benchmarks',
    'nav.blog': 'Blog',
    'nav.toolkit': 'Toolkit',
    'nav.resources': 'Resources',
    'nav.faq': 'FAQ',
    'nav.demo': 'Demo',
    
    // Home
    'hero.title': 'Gemma-3n.net: The Ultimate Developer Guide',
    'hero.subtitle': 'Master Google\'s Gemma 3n AI models with comprehensive tutorials, benchmarks, and tools.',
    'hero.cta.primary': 'Get Started',
    'hero.cta.secondary': 'View Demos',
    
    // Demo
    'demo.title': 'üöÄ Gemma 3n Interactive Demo',
    'demo.subtitle': 'Experience in-browser AI inference - fully local, no server required',
    'demo.loading.initializing': 'Initializing lightweight AI model...',
    'demo.loading.engine': 'Downloading lightweight inference engine...',
    'demo.loading.wasm': 'Initializing WebAssembly module...',
    'demo.loading.vocab': 'Loading vocabulary and configuration...',
    'demo.loading.weights': 'Optimizing model weights...',
    'demo.loading.ready': 'Model ready!',
    'demo.status.ready': '‚úÖ Gemma 3n Lite is ready (Online Demo Version)',
    'demo.scenarios.label': 'Select a demo scenario',
    'demo.scenarios.code.title': 'Code Completion',
    'demo.scenarios.code.description': 'Intelligent code suggestions',
    'demo.scenarios.translate.title': 'Language Translation',
    'demo.scenarios.translate.description': 'Translate between languages',
    'demo.scenarios.chat.title': 'Assistant Chat',
    'demo.scenarios.chat.description': 'Conversational assistant',
    'demo.input.label': 'Input Text',
    'demo.input.placeholder': 'Enter your text...',
    'demo.parameters.label': 'Temperature (Creativity)',
    'demo.parameters.conservative': 'Conservative',
    'demo.parameters.creative': 'Creative',
    'demo.button.loading': 'Model loading...',
    'demo.button.generate': 'Generate AI Response',
    'demo.button.generating': 'Generating...',
    'demo.output.label': 'AI Output',
    'demo.output.placeholder': 'AI-generated content will appear here...',
    'demo.output.thinking': 'ü§ñ AI is thinking...',
    'demo.output.error': '‚ùå Generation failed:',
    'demo.alert.no_input': 'Please enter some text',
    'demo.metrics.tokensPerSecond': 'Tokens/sec',
    'demo.metrics.inferenceTime': 'Inference Time (ms)',
    'demo.metrics.memoryUsage': 'Memory Usage (MB)',
    'demo.metrics.modelSize': 'Model Size',
    'demo.placeholders.code': `function fibonacci(n) {
  // Calculate fibonacci sequence
  if (n <= 1) return n;
  return`,
    'demo.placeholders.translate': `Please translate the following text to English:
Artificial intelligence is changing our world.`,
    'demo.placeholders.chat': `Hello, I would like to know the features and advantages of the Gemma 3n model.`,
    'demo.simulations.code_response': ` fibonacci(n - 1) + fibonacci(n - 2);
}

// Optimized version: using dynamic programming
function fibonacciDP(n) {
  const dp = [0, 1];
  for (let i = 2; i <= n; i++) {
    dp[i] = dp[i-1] + dp[i-2];
  }
  return dp[n];
}`,
    'demo.simulations.translation_response': `**Translation Result:**
Artificial Intelligence is transforming our world.

*Detected Language: Chinese ‚Üí English*`,
    'demo.simulations.chat_response_1': `Hello! I'm happy to introduce Gemma 3n. It is the latest multimodal AI model released by Google, with the following features:

üéØ **On-device Optimization**: Designed for devices like mobile phones and laptops
‚ö° **Efficient Architecture**: "Nested" MatFormer Transformer structure
üé® **Multimodal Capability**: Supports text, image, and audio processing
üîß **Developer Friendly**: Compatible with the mainstream AI tool ecosystem

Which aspect would you like to know more about?`,
    'demo.simulations.chat_response_2': `The advantages of Gemma 3n compared to other models are:

1. **Smaller Memory Footprint**: The E2B version requires only 4GB of memory
2. **Faster Inference Speed**: Optimized for mobile devices
3. **Better Accuracy**: Achieves 79.8% on the MMLU benchmark
4. **Stronger Multimodality**: Natively supports vision and audio

This makes it particularly suitable for building privacy-first on-device AI applications.`,
    'demo.simulations.chat_response_3': `Regarding the technical implementation of Gemma 3n, I recommend that you:

üìö **Learning Path**: Start with our introductory tutorials
üíª **Practical Projects**: Try the iOS deployment or Ollama integration
üîß **Advanced Skills**: Learn LoRA fine-tuning techniques
üåê **Community Involvement**: Follow the latest model updates and optimizations

Would you like me to recommend a specific tutorial?`,
    
    // Controls
    'controls.temperature': 'Temperature',
    'controls.temperature.desc': 'Controls randomness in output',
    'controls.maxTokens': 'Max Tokens',
    'controls.maxTokens.desc': 'Maximum length of generated text',
    'controls.topK': 'Top-K',
    'controls.topK.desc': 'Limits vocabulary for more focused output',
    
    // Metrics
    'metrics.inference': 'Inference Time',
    'metrics.memory': 'Memory Usage',
    'metrics.tokens': 'Tokens/sec',
    'metrics.local': 'Local Processing',
    'metrics.api': 'API Call',
    
    // Scenarios
    'scenarios.code': 'Code Generation',
    'scenarios.code.desc': 'Generate and complete code',
    'scenarios.text': 'Text Generation', 
    'scenarios.text.desc': 'Creative writing and content',
    'scenarios.chat': 'Q&A Chat',
    'scenarios.chat.desc': 'Interactive conversations',
    
    // Benchmarks
    'benchmarks.title': 'Performance Benchmarks',
    'benchmarks.subtitle': 'How does Gemma 3n compare to competitors? Here are the benchmarks.',
    'benchmarks.source': 'Data sourced from official Google AI publications and independent benchmarks.',
    'benchmarks.heading': 'Performance Benchmarks',
    'benchmarks.efficiency_champion_title': 'Efficiency Champion',
    'benchmarks.efficiency_champion': 'Gemma 3n E4B achieves 79.8% MMLU with only 4B parameters, outperforming Llama 3.1 8B (66.7%) while using half the memory.',
    'benchmarks.mobile_first_design_title': 'Mobile-First Design',
    'benchmarks.mobile_first_design': 'MatFormer architecture enables dynamic scaling, allowing the same model to run efficiently from smartphones to workstations.',
    'benchmarks.mmlu.title': 'MMLU',
    'benchmarks.mmlu.desc': 'Massive Multitask Language Understanding',
    'benchmarks.mmlu.note': 'Outperforms leading models in its class on this key knowledge and reasoning benchmark.',
    'benchmarks.lmarena.title': 'LMArena Score',
    'benchmarks.lmarena.desc': 'Human preference chatbot benchmark',
    'benchmarks.lmarena.max': 'of max observed',
    'benchmarks.lmarena.note': 'The first model under 10B parameters to break the 1300 barrier, showcasing strong conversational ability.',
    'benchmarks.vision.title': 'Vision Encoder Speed',
    'benchmarks.vision.desc': 'On-device performance (Pixel Edge TPU)',
    'benchmarks.vision.faster': 'Faster',
    'benchmarks.vision.compare': 'MobileNet-V5 vs SoViT',
    'benchmarks.vision.note': 'A massive speedup in vision processing with higher accuracy and a smaller memory footprint.',
    'benchmarks.table.title': 'Gemma 3n vs. Competition',
    'benchmarks.table.model': 'Model',
    'benchmarks.table.params': 'Parameters',
    'benchmarks.table.mmlu': 'MMLU',
    'benchmarks.table.gsm8k': 'GSM8K',
    'benchmarks.table.humaneval': 'HumanEval',
    'benchmarks.table.memory': 'Memory (GB)',
    'benchmarks.gemma_e4b': 'Gemma 3n E4B',
    'benchmarks.gemma_e2b': 'Gemma 3n E2B',
    'benchmarks.llama_3_8b': 'Llama 3.1 8B',
    'benchmarks.llama_3_3b': 'Llama 3.2 3B',
    'benchmarks.score': 'Score',
    'benchmarks.tag.superior': 'Superior performance',
    'benchmarks.tag.below': 'Below Gemma 3n E4B',
    'benchmarks.memory.note': 'Memory requirements are for full precision models.',

    // Toolkit
    'toolkit.title': 'Developer Toolkit',
    'toolkit.subtitle': 'Everything you need to get started with Gemma 3n',
    'toolkit.quickstart': 'Quick Start',
    'toolkit.integration': 'Platform Integration',
    'toolkit.requirements': 'Hardware Requirements',
    'toolkit.practices': 'Best Practices',
    
    // Common
    'common.loading': 'Loading...',
    'common.error': 'Error',
    'common.retry': 'Try Again',
    'common.close': 'Close',
    'common.save': 'Save',
    'common.cancel': 'Cancel',
    'common.continue': 'Continue',
    'common.learn.more': 'Learn More',
    'common.get.started': 'Get Started',
    'common.view.all': 'View All',
    
    // PWA
    'pwa.install': 'Install App',
    'pwa.install.desc': 'Install Gemma-3n.net for offline access',
    'pwa.update': 'Update Available',
    'pwa.update.desc': 'A new version is available. Refresh to update.',
    'pwa.offline': 'You\'re offline',
    'pwa.offline.desc': 'Some features may be limited without internet connection.',
    
    // ËØ≠Ë®ÄÂàáÊç¢
    'lang.switch': 'Switch Language',
    'lang.current': 'English',

    // resources
    'resources.heading': 'Resources',
    'resources.subtitle': 'Essential links to get you started and building with Gemma 3n.',
    'resources.category.download': 'Download Models',
    'resources.category.api': 'Official APIs & Guides',
    'resources.download.hf.title': 'Hugging Face',
    'resources.download.hf.desc': 'Access all Gemma 3n models, GGUF versions, and community-tuned variants.',
    'resources.download.ollama.title': 'Ollama',
    'resources.download.ollama.desc': 'Pull and run Gemma 3n models with a single command on your local machine.',
    'resources.download.lmstudio.title': 'LM Studio',
    'resources.download.lmstudio.desc': 'Discover and run Gemma 3n through a user-friendly desktop application.',
    'resources.api.studio.title': 'Google AI Studio',
    'resources.api.studio.desc': 'Experiment with Gemma 3n models directly in your browser with Google\'s free tool.',
    'resources.api.keras.title': 'Keras',
    'resources.api.keras.desc': 'Utilize Gemma 3n with Keras, Google\'s high-level API for building and training models.',
    'resources.api.paper.title': 'Technical Report',
    'resources.api.paper.desc': 'Read the official paper from Google for a deep dive into Gemma 3n\'s architecture.',

    // Demo È°µÈù¢‰∏ªÁªìÊûÑ
    'demo.page.title': 'AI Demo | Interactive Experience',
    'demo.page.description': 'Experience the power of Gemma 3n - run AI models directly in your browser, no installation required.',
    'demo.hero.title': 'Gemma 3n Interactive Experience',
    'demo.hero.subtitle': 'Experience powerful AI features directly in your browser. Code completion ‚Ä¢ Language translation ‚Ä¢ Intelligent Q&A',
    'demo.hero.feature1.title': 'Ultra-fast Response',
    'demo.hero.feature1.desc': 'Millisecond-level AI inference, real-time interaction',
    'demo.hero.feature2.title': 'Privacy First',
    'demo.hero.feature2.desc': 'All data processed locally, never uploaded to the cloud',
    'demo.hero.feature3.title': 'Multi-scenario Support',
    'demo.hero.feature3.desc': 'Coding, translation, chat ‚Äî one model for all',
    'demo.hero.cta': 'Try Now ‚Üí',
    'demo.section.title': 'Interactive AI Demo',
    'demo.section.desc': 'This is a simulated version showing how Gemma 3n works in real-world scenarios. For production, use ONNX.js or WebAssembly to run the real model.',
    'demo.info.title': 'About this Demo',
    'demo.info.current.title': 'Current Features',
    'demo.info.current.feature1': 'Simulates Gemma 3n inference process and response style',
    'demo.info.current.feature2': 'Realistic UI and interaction flow',
    'demo.info.current.feature3': 'Performance metrics based on real hardware data',
    'demo.info.current.feature4': 'Supports three core application scenarios',
    'demo.info.prod.title': 'Production Version',
    'demo.info.prod.feature1': 'Load real Gemma 3n model with ONNX.js',
    'demo.info.prod.feature2': 'Accelerated inference with WebAssembly',
    'demo.info.prod.feature3': 'Full tokenizer and post-processing pipeline',
    'demo.info.prod.feature4': 'Supports model quantization and optimization',
    'demo.tech.title': 'Technical Implementation Path',
    'demo.tech.desc': 'Upgrade the demo to a full-fledged AI application tech stack',
    'demo.tech.frontend.title': 'Frontend Architecture',
    'demo.tech.frontend.engine': 'Lightweight Inference Engine',
    'demo.tech.frontend.engine.code': "// ONNX.js integration\nimport * as ort from 'onnxruntime-web';\n\n// Load model\nconst session = await ort.InferenceSession\n  .create('/models/gemma-3n-e2b.onnx');\n\n// Inference\nconst results = await session.run(feeds);",
    'demo.tech.frontend.wasm': 'WebAssembly Optimization',
    'demo.tech.frontend.wasm.code': "// WebAssembly tokenizer\nimport init, { tokenize } from './pkg/tokenizer.js';\n\n// Initialize WASM module\nawait init();\n\n// High-performance tokenization\nconst tokens = tokenize(inputText);",
    'demo.tech.deploy.title': 'Model Deployment',
    'demo.tech.deploy.convert': 'Model Conversion',
    'demo.tech.deploy.convert1': 'Hugging Face ‚Üí ONNX',
    'demo.tech.deploy.convert2': 'Dynamic quantization (INT8)',
    'demo.tech.deploy.convert3': 'Graph optimization and constant folding',
    'demo.tech.deploy.convert4': 'WebGL backend adaptation',
    'demo.tech.deploy.cdn': 'CDN Distribution',
    'demo.tech.deploy.cdn1': 'Global acceleration with Cloudflare',
    'demo.tech.deploy.cdn2': 'Chunked download strategy',
    'demo.tech.deploy.cdn3': 'Browser cache optimization',
    'demo.tech.deploy.cdn4': 'Progressive loading',
    'demo.tech.deploy.optimize': 'Performance Optimization',
    'demo.tech.deploy.optimize1': 'Web Workers multithreading',
    'demo.tech.deploy.optimize2': 'SharedArrayBuffer',
    'demo.tech.deploy.optimize3': 'WebGPU acceleration (future)',
    'demo.tech.deploy.optimize4': 'Memory pool management',
    'demo.tech.cost.title': 'Zero-cost Solution Advantages',
    'demo.tech.cost.cloud': 'Traditional Cloud AI Cost',
    'demo.tech.cost.cloud1': 'üî¥ OpenAI API: $0.002/1K tokens',
    'demo.tech.cost.cloud2': 'üî¥ Azure OpenAI: $0.0015/1K tokens',
    'demo.tech.cost.cloud3': 'üî¥ Google Cloud AI: $0.001/1K tokens',
    'demo.tech.cost.cloud4': 'üî¥ Monthly: $200-2000 (medium traffic)',
    'demo.tech.cost.device': 'Gemma 3n On-device Solution',
    'demo.tech.cost.device1': '‚úÖ Inference cost: $0',
    'demo.tech.cost.device2': '‚úÖ CDN: $0 (Cloudflare free tier)',
    'demo.tech.cost.device3': '‚úÖ Storage: $0 (static hosting)',
    'demo.tech.cost.device4': '‚úÖ Monthly: $0 + $12/year domain',
    'demo.cta.title': 'Ready to build your AI app?',
    'demo.cta.desc': 'Start with tutorials and master the power of Gemma 3n step by step.',
    'demo.cta.learn': 'Start Learning',
    'demo.cta.toolkit': 'Toolkit',
    'demo.cta.download': 'Download Model',

    // FAQÂå∫Âùó
    'faq.heading': 'Frequently Asked Questions',
    'faq.subtitle': "Got questions? We've got answers. Here are some of the most common things developers ask about Gemma 3n.",
    'faq.q1': 'Is Gemma 3n free to use?',
    'faq.a1': 'Yes, Gemma 3n models are released under a license that permits free access for commercial and research use. Always check the official license terms for details.',
    'faq.q2': "What does 'multimodal' actually mean for Gemma 3n?",
    'faq.a2': 'It means the model can natively understand and process more than just text. It can analyze images and listen to audio, making it suitable for a wider range of applications like describing photos or transcribing speech.',
    'faq.q3': 'How is Gemma 3n different from the regular Gemma 2 or Gemma family?',
    'faq.a3': 'Gemma 3n is specifically optimized for on-device performance. It uses the novel MatFormer architecture to be more efficient in terms of memory and computation, making it ideal for running on phones and laptops.',
    'faq.q4': 'Can I fine-tune Gemma 3n on my own data?',
    'faq.a4': 'Absolutely. The models are designed to be fine-tuned. Google provides recipes and support through frameworks like Keras, PyTorch, and JAX to facilitate this process.',
    'faq.q5': 'How do I run Gemma 3n with Ollama?',
    'faq.a5': "Running Gemma 3n with Ollama is straightforward. Simply install Ollama and run 'ollama run gemma-3n:e4b' for the 4B model or 'ollama run gemma-3n:e2b' for the 2B model. The model will be downloaded automatically.",
    'faq.q6': "What's the difference between Gemma 3n E2B and E4B models?",
    'faq.a6': 'E2B (2B parameters) is smaller and faster, ideal for mobile devices and quick inference. E4B (4B parameters) offers better performance and accuracy but requires more computational resources. Both use the same MatFormer architecture.',
    'faq.q7': 'Can I use Gemma 3n models from Hugging Face?',
    'faq.a7': 'Yes, all Gemma 3n models are available on Hugging Face Hub. You can use them with the transformers library: from transformers import AutoModelForCausalLM, AutoTokenizer. Both 16-bit and quantized versions are available.',
    'faq.q8': 'Is Gemma 3n better than Llama 3 for local AI setups?',
    'faq.a8': "Gemma 3n E4B often outperforms Llama 3 8B in many benchmarks while being smaller and more efficient. For on-device applications, Gemma 3n's MatFormer architecture provides better memory efficiency and faster inference.",
    'faq.q9': 'Can I run Gemma 3n on iOS devices?',
    'faq.a9': 'Yes, Gemma 3n models can run on iOS devices. The E2B model is particularly well-suited for mobile deployment. You can use frameworks like CoreML or run quantized versions for optimal performance on Apple devices.',
    'faq.q10': 'What hardware requirements does Gemma 3n have?',
    'faq.a10': 'Gemma 3n E2B can run on devices with as little as 4GB RAM. E4B typically requires 8GB+ RAM for comfortable operation. Both models can run on CPU-only setups, though GPU acceleration significantly improves inference speed.',
    
    // newsletter
    'newsletter.heading': 'Stay Updated',
    'newsletter.desc': 'Get the latest tutorials and news about Gemma 3n delivered to your inbox. No spam, ever.',
    'newsletter.placeholder': 'Enter your email',
    'newsletter.subscribe': 'Subscribe',

    // footer
    'footer.rights': 'All rights reserved.',
    'footer.independent': 'An independent project, not affiliated with Google.',  

    // Blog
    'blog.title': 'Blog',
    'blog.description': 'In-depth articles, tutorials, and analysis on Gemma 3n and the world of on-device AI.',
    'blog.heading': 'Blog & Tutorials',
    'blog.subtitle': 'In-depth articles, guides, and analysis to help you master on-device AI.',
    'blog.posted_on': 'Posted on',
  },
  zh: {
    // ÂØºËà™
    'nav.home': 'È¶ñÈ°µ',
    'nav.about': 'ÂÖ≥‰∫é',
    'nav.benchmarks': 'ÊÄßËÉΩÊµãËØï',
    'nav.blog': 'ÂçöÂÆ¢',
    'nav.toolkit': 'Â∑•ÂÖ∑ÂåÖ',
    'nav.resources': 'ËµÑÊ∫ê',
    'nav.faq': 'Â∏∏ËßÅÈóÆÈ¢ò',
    'nav.demo': 'ÊºîÁ§∫',
    
    // È¶ñÈ°µ
    'hero.title': 'Gemma-3n.net: ÁªàÊûÅÂºÄÂèëËÄÖÊåáÂçó',
    'hero.subtitle': 'ÈÄöËøáÂÖ®Èù¢ÁöÑÊïôÁ®ã„ÄÅÂü∫ÂáÜÊµãËØïÂíåÂ∑•ÂÖ∑ÔºåÊéåÊè°GoogleÁöÑGemma 3n AIÊ®°Âûã„ÄÇ',
    'hero.cta.primary': 'ÂºÄÂßã‰ΩøÁî®',
    'hero.cta.secondary': 'Êü•ÁúãÊºîÁ§∫',
    
    // Demo
    'demo.title': 'üöÄ Gemma 3n ‰∫§‰∫íÂºèDemo',
    'demo.subtitle': '‰ΩìÈ™åÊµèËßàÂô®ÂÜÖAIÊé®ÁêÜ - ÂÆåÂÖ®Êú¨Âú∞ÂåñÔºåÊó†ÈúÄÊúçÂä°Âô®',
    'demo.loading.initializing': 'Ê≠£Âú®ÂàùÂßãÂåñËΩªÈáèÁ∫ßAIÊ®°Âûã...',
    'demo.loading.engine': '‰∏ãËΩΩËΩªÈáèÁ∫ßÊé®ÁêÜÂºïÊìé...',
    'demo.loading.wasm': 'ÂàùÂßãÂåñWebAssemblyÊ®°Âùó...',
    'demo.loading.vocab': 'Âä†ËΩΩËØçÊ±áË°®ÂíåÈÖçÁΩÆ...',
    'demo.loading.weights': '‰ºòÂåñÊ®°ÂûãÊùÉÈáç...',
    'demo.loading.ready': 'Ê®°ÂûãÂ∞±Áª™ÔºÅ',
    'demo.status.ready': '‚úÖ Gemma 3n Lite Â∑≤Â∞±Áª™ (Âú®Á∫øDemoÁâàÊú¨)',
    'demo.scenarios.label': 'ÈÄâÊã©ÊºîÁ§∫Âú∫ÊôØ',
    'demo.scenarios.code.title': '‰ª£Á†ÅË°•ÂÖ®',
    'demo.scenarios.code.description': 'Êô∫ËÉΩ‰ª£Á†ÅÂª∫ËÆÆ',
    'demo.scenarios.translate.title': 'ËØ≠Ë®ÄÁøªËØë',
    'demo.scenarios.translate.description': 'Â§öËØ≠Ë®Ä‰∫íËØë',
    'demo.scenarios.chat.title': 'Êô∫ËÉΩÈóÆÁ≠î',
    'demo.scenarios.chat.description': 'ÂØπËØùÂä©Êâã',
    'demo.input.label': 'ËæìÂÖ•ÊñáÊú¨',
    'demo.input.placeholder': 'ËæìÂÖ•ÊÇ®ÁöÑÊñáÊú¨...',
    'demo.parameters.label': 'Ê∏©Â∫¶ËÆæÁΩÆ (ÂàõÈÄ†ÊÄß)',
    'demo.parameters.conservative': '‰øùÂÆà',
    'demo.parameters.creative': 'ÂàõÊñ∞',
    'demo.button.loading': 'Ê®°ÂûãÂä†ËΩΩ‰∏≠...',
    'demo.button.generate': 'ÁîüÊàêAIÂõûÂ§ç',
    'demo.button.generating': 'ÁîüÊàê‰∏≠...',
    'demo.output.label': 'AIËæìÂá∫ÁªìÊûú',
    'demo.output.placeholder': 'AIÁîüÊàêÁöÑÂÜÖÂÆπÂ∞ÜÂú®ËøôÈáåÊòæÁ§∫...',
    'demo.output.thinking': 'ü§ñ AIÊ≠£Âú®ÊÄùËÄÉ...',
    'demo.output.error': '‚ùå ÁîüÊàêÂ§±Ë¥•:',
    'demo.alert.no_input': 'ËØ∑ËæìÂÖ•‰∏Ä‰∫õÊñáÊú¨',
    'demo.metrics.tokensPerSecond': 'Tokens/Áßí',
    'demo.metrics.inferenceTime': 'Êé®ÁêÜÊó∂Èó¥(ms)',
    'demo.metrics.memoryUsage': 'ÂÜÖÂ≠ò‰ΩøÁî®(MB)',
    'demo.metrics.modelSize': 'Ê®°ÂûãÂ§ßÂ∞è',
    'demo.placeholders.code': `function fibonacci(n) {
  // ËÆ°ÁÆóÊñêÊ≥¢ÈÇ£Â•ëÊï∞Âàó
  if (n <= 1) return n;
  return`,
    'demo.placeholders.translate': `ËØ∑Â∞Ü‰ª•‰∏ãÊñáÊú¨ÁøªËØëÊàêËã±ÊñáÔºö
‰∫∫Â∑•Êô∫ËÉΩÊ≠£Âú®ÊîπÂèòÊàë‰ª¨ÁöÑ‰∏ñÁïå„ÄÇ`,
    'demo.placeholders.chat': `‰Ω†Â•ΩÔºåÊàëÊÉ≥‰∫ÜËß£Gemma 3nÊ®°ÂûãÁöÑÁâπÁÇπÂíå‰ºòÂäø„ÄÇ`,
    'demo.simulations.code_response': ` fibonacci(n - 1) + fibonacci(n - 2);
}

// ‰ºòÂåñÁâàÊú¨Ôºö‰ΩøÁî®Âä®ÊÄÅËßÑÂàí
function fibonacciDP(n) {
  const dp = [0, 1];
  for (let i = 2; i <= n; i++) {
    dp[i] = dp[i-1] + dp[i-2];
  }
  return dp[n];
}`,
    'demo.simulations.translation_response': `**ÁøªËØëÁªìÊûú:**
‰∫∫Â∑•Êô∫ËÉΩÊ≠£Âú®ÊîπÂèòÊàë‰ª¨ÁöÑ‰∏ñÁïå„ÄÇ

*Ê£ÄÊµãÂà∞ÁöÑËØ≠Ë®Ä: ‰∏≠Êñá ‚Üí Ëã±Êñá*`,
    'demo.simulations.chat_response_1': `ÊÇ®Â•ΩÔºÅÊàëÂæàÈ´òÂÖ¥‰∏∫ÊÇ®‰ªãÁªçGemma 3n„ÄÇËøôÊòØGoogleÊúÄÊñ∞ÂèëÂ∏ÉÁöÑÂ§öÊ®°ÊÄÅAIÊ®°ÂûãÔºåÂÖ∑Êúâ‰ª•‰∏ãÁâπÁÇπÔºö

üéØ **ËÆæÂ§áÁ´Ø‰ºòÂåñ**: ‰∏ì‰∏∫ÊâãÊú∫„ÄÅÁ¨îËÆ∞Êú¨Á≠âËÆæÂ§áËÆæËÆ°
‚ö° **È´òÊïàÊû∂ÊûÑ**: MatFormer"ÂµåÂ•ó"TransformerÁªìÊûÑ
üé® **Â§öÊ®°ÊÄÅËÉΩÂäõ**: ÊîØÊåÅÊñáÊú¨„ÄÅÂõæÂÉè„ÄÅÈü≥È¢ëÂ§ÑÁêÜ
üîß **ÂºÄÂèëËÄÖÂèãÂ•Ω**: ‰∏é‰∏ªÊµÅAIÂ∑•ÂÖ∑ÁîüÊÄÅÂÖºÂÆπ

ÊÇ®ËøòÊÉ≥‰∫ÜËß£Âì™‰∏™ÊñπÈù¢ÁöÑËØ¶ÁªÜ‰ø°ÊÅØÔºü`,
    'demo.simulations.chat_response_2': `Gemma 3nÁõ∏ÊØîÂÖ∂‰ªñÊ®°ÂûãÁöÑ‰ºòÂäøÂú®‰∫éÔºö

1. **Êõ¥Â∞èÁöÑÂÜÖÂ≠òÂç†Áî®**: E2BÁâàÊú¨‰ªÖÈúÄ4GBÂÜÖÂ≠ò
2. **Êõ¥Âø´ÁöÑÊé®ÁêÜÈÄüÂ∫¶**: ÈíàÂØπÁßªÂä®ËÆæÂ§á‰ºòÂåñ
3. **Êõ¥Â•ΩÁöÑÂáÜÁ°ÆÊÄß**: MMLUÂü∫ÂáÜÊµãËØïËææÂà∞79.8%
4. **Êõ¥Âº∫ÁöÑÂ§öÊ®°ÊÄÅ**: ÂéüÁîüÊîØÊåÅËßÜËßâÂíåÈü≥È¢ë

Ëøô‰ΩøÂæóÂÆÉÁâπÂà´ÈÄÇÂêàÊûÑÂª∫ÈöêÁßÅ‰ºòÂÖàÁöÑËÆæÂ§áÁ´ØAIÂ∫îÁî®„ÄÇ`,
    'demo.simulations.chat_response_3': `ÂÖ≥‰∫éGemma 3nÁöÑÊäÄÊúØÂÆûÁé∞ÔºåÊàëÂª∫ËÆÆÊÇ®Ôºö

üìö **Â≠¶‰π†Ë∑ØÂæÑ**: ‰ªéÊàë‰ª¨ÁöÑÂÖ•Èó®ÊïôÁ®ãÂºÄÂßã
üíª **ÂÆûË∑µÈ°πÁõÆ**: Â∞ùËØïiOSÈÉ®ÁΩ≤ÊàñOllamaÈõÜÊàê
üîß **ËøõÈò∂ÊäÄËÉΩ**: Â≠¶‰π†LoRAÂæÆË∞ÉÊäÄÊúØ
üåê **Á§æÂå∫ÂèÇ‰∏é**: ÂÖ≥Ê≥®ÊúÄÊñ∞ÁöÑÊ®°ÂûãÊõ¥Êñ∞Âíå‰ºòÂåñ

ÈúÄË¶ÅÊàë‰∏∫ÊÇ®Êé®ËçêÂÖ∑‰ΩìÁöÑÊïôÁ®ãÂêóÔºü`,
    
    // ÂèÇÊï∞ÊéßÂà∂
    'controls.temperature': 'Ê∏©Â∫¶ÂÄº',
    'controls.temperature.desc': 'ÊéßÂà∂ËæìÂá∫ÁöÑÈöèÊú∫ÊÄß',
    'controls.maxTokens': 'ÊúÄÂ§ßÈïøÂ∫¶',
    'controls.maxTokens.desc': 'ÁîüÊàêÊñáÊú¨ÁöÑÊúÄÂ§ßÈïøÂ∫¶',
    'controls.topK': 'Top-K',
    'controls.topK.desc': 'ÈôêÂà∂ËØçÊ±áË°®‰ª•Ëé∑ÂæóÊõ¥ÈõÜ‰∏≠ÁöÑËæìÂá∫',
    
    // ÊÄßËÉΩÁõëÊéß
    'metrics.inference': 'Êé®ÁêÜÊó∂Èó¥',
    'metrics.memory': 'ÂÜÖÂ≠ò‰ΩøÁî®',
    'metrics.tokens': '‰ª§Áâå/Áßí',
    'metrics.local': 'Êú¨Âú∞Â§ÑÁêÜ',
    'metrics.api': 'APIË∞ÉÁî®',
    
    // È¢ÑËÆæÂú∫ÊôØ
    'scenarios.code': '‰ª£Á†ÅÁîüÊàê',
    'scenarios.code.desc': 'ÁîüÊàêÂíåË°•ÂÖ®‰ª£Á†Å',
    'scenarios.text': 'ÊñáÊú¨ÁîüÊàê',
    'scenarios.text.desc': 'ÂàõÊÑèÂÜô‰ΩúÂíåÂÜÖÂÆπÂàõ‰Ωú',
    'scenarios.chat': 'ÈóÆÁ≠îÂØπËØù',
    'scenarios.chat.desc': '‰∫§‰∫íÂºèÂØπËØù',
    
    // Â∑•ÂÖ∑ÂåÖ
    'toolkit.title': 'ÂºÄÂèëËÄÖÂ∑•ÂÖ∑ÂåÖ',
    'toolkit.subtitle': 'ÂºÄÂßã‰ΩøÁî® Gemma 3n ÊâÄÈúÄÁöÑ‰∏ÄÂàá',
    'toolkit.quickstart': 'Âø´ÈÄüÂºÄÂßã',
    'toolkit.integration': 'Âπ≥Âè∞ÈõÜÊàê',
    'toolkit.requirements': 'Á°¨‰ª∂Ë¶ÅÊ±Ç',
    'toolkit.practices': 'ÊúÄ‰Ω≥ÂÆûË∑µ',
    
    // ÈÄöÁî®
    'common.loading': 'Âä†ËΩΩ‰∏≠...',
    'common.error': 'ÈîôËØØ',
    'common.retry': 'ÈáçËØï',
    'common.close': 'ÂÖ≥Èó≠',
    'common.save': '‰øùÂ≠ò',
    'common.cancel': 'ÂèñÊ∂à',
    'common.continue': 'ÁªßÁª≠',
    'common.learn.more': '‰∫ÜËß£Êõ¥Â§ö',
    'common.get.started': 'ÂºÄÂßã‰ΩøÁî®',
    'common.view.all': 'Êü•ÁúãÂÖ®ÈÉ®',
    
    // PWA
    'pwa.install': 'ÂÆâË£ÖÂ∫îÁî®',
    'pwa.install.desc': 'ÂÆâË£Ö Gemma-3n.net ‰ª•‰æøÁ¶ªÁ∫øËÆøÈóÆ',
    'pwa.update': 'ÊúâÂèØÁî®Êõ¥Êñ∞',
    'pwa.update.desc': 'Êñ∞ÁâàÊú¨ÂèØÁî®ÔºåÂà∑Êñ∞È°µÈù¢‰ª•Êõ¥Êñ∞„ÄÇ',
    'pwa.offline': 'ÊÇ®ÂΩìÂâçÁ¶ªÁ∫ø',
    'pwa.offline.desc': 'Ê≤°ÊúâÁΩëÁªúËøûÊé•Êó∂Êüê‰∫õÂäüËÉΩÂèØËÉΩÂèóÈôê„ÄÇ',

    // ËØ≠Ë®ÄÂàáÊç¢
    'lang.switch': 'ÂàáÊç¢ËØ≠Ë®Ä',
    'lang.current': '‰∏≠Êñá',

    // Âü∫ÂáÜÊµãËØï
    'benchmarks.title': 'ÊÄßËÉΩÂü∫ÂáÜ',
    'benchmarks.subtitle': 'Gemma 3n ‰∏éÁ´û‰∫âÂØπÊâãÁõ∏ÊØîË°®Áé∞Â¶Ç‰ΩïÔºüËøôÈáåÊòØÊï∞ÊçÆÂØπÊØî„ÄÇ',
    'benchmarks.source': 'Êï∞ÊçÆÊù•Ê∫ê‰∫éGoogle AIÂÆòÊñπÂèëÂ∏ÉÂíåÁã¨Á´ãÂü∫ÂáÜÊµãËØï„ÄÇ',
    'benchmarks.heading': 'ÊÄßËÉΩÂü∫ÂáÜ',
    'benchmarks.mmlu.title': 'MMLU',
    'benchmarks.mmlu.desc': 'Â§ö‰ªªÂä°ËØ≠Ë®ÄÁêÜËß£ËÉΩÂäõ',
    'benchmarks.mmlu.note': 'Âú®ËØ•ÂÖ≥ÈîÆÁü•ËØÜ‰∏éÊé®ÁêÜÂü∫ÂáÜ‰∏äË∂ÖË∂äÂêåÁ∫ß‰∏ªÊµÅÊ®°Âûã',
    'benchmarks.lmarena.title': 'LMArena ÂæóÂàÜ',
    'benchmarks.lmarena.desc': '‰∫∫Á±ªÂÅèÂ•ΩÂØπËØùÂü∫ÂáÜ',
    'benchmarks.lmarena.max': 'ÊúÄÂ§ßÂàÜÊï∞Âç†ÊØî',
    'benchmarks.lmarena.note': 'È¶ñ‰∏™Á™ÅÁ†¥1300ÂàÜÁöÑ10B‰ª•‰∏ãÊ®°ÂûãÔºåÂ±ïÁé∞Âº∫Â§ßÂØπËØùËÉΩÂäõ',
    'benchmarks.vision.title': 'ËßÜËßâÁºñÁ†ÅÈÄüÂ∫¶',
    'benchmarks.vision.desc': 'ËÆæÂ§áÁ´ØËßÜËßâÊé®ÁêÜÊÄßËÉΩÔºàPixel Edge TPUÔºâ',
    'benchmarks.vision.faster': 'Âä†ÈÄü',
    'benchmarks.vision.compare': 'MobileNet-V5 vs SoViT',
    'benchmarks.vision.note': 'ËßÜËßâÂ§ÑÁêÜÈÄüÂ∫¶Â§ßÂπÖÊèêÂçáÔºåÁ≤æÂ∫¶Êõ¥È´òÔºåÂÜÖÂ≠òÂç†Áî®Êõ¥‰Ωé',
    'benchmarks.table.title': 'Gemma 3n ‰∏éÁ´ûÂìÅÂØπÊØî',
    'benchmarks.table.model': 'Ê®°Âûã',
    'benchmarks.table.params': 'ÂèÇÊï∞Èáè',
    'benchmarks.table.mmlu': 'MMLU',
    'benchmarks.table.gsm8k': 'GSM8K',
    'benchmarks.table.humaneval': 'HumanEval',
    'benchmarks.table.memory': 'ÂÜÖÂ≠ò(GB)',
    'benchmarks.gemma_e4b': 'Gemma 3n E4B',
    'benchmarks.gemma_e2b': 'Gemma 3n E2B',
    'benchmarks.llama_3_8b': 'Llama 3.1 8B',
    'benchmarks.llama_3_3b': 'Llama 3.2 3B',
    'benchmarks.score': 'ÂàÜÊï∞',
    'benchmarks.tag.superior': 'ÊÄßËÉΩÈ¢ÜÂÖà',
    'benchmarks.tag.below': '‰Ωé‰∫é Gemma 3n E4B',
    'benchmarks.memory.note': 'ÂÜÖÂ≠òÈúÄÊ±Ç‰∏∫ÂÖ®Á≤æÂ∫¶Ê®°ÂûãÈÖçÁΩÆ„ÄÇ',
    'benchmarks.efficiency_champion_title': 'ÊïàÁéá‰πãÁéã',
    'benchmarks.efficiency_champion': 'Gemma 3n E4B ‰ªÖÁî® 4B ÂèÇÊï∞Âç≥Ëææ 79.8% MMLUÔºåÊÄßËÉΩË∂ÖË∂ä Llama 3.1 8BÔºà66.7%ÔºâÔºåÂÜÖÂ≠òÂç†Áî®‰ªÖ‰∏∫ÂÖ∂‰∏ÄÂçä„ÄÇ',
    'benchmarks.mobile_first_design_title': 'ÁßªÂä®‰ºòÂÖàËÆæËÆ°',
    'benchmarks.mobile_first_design': 'MatFormer Êû∂ÊûÑÊîØÊåÅÂä®ÊÄÅÊâ©Â±ïÔºåÂêå‰∏ÄÊ®°ÂûãÂèØÈ´òÊïàËøêË°å‰∫éÊâãÊú∫Âà∞Â∑•‰ΩúÁ´ôÁ≠âÂ§öÁßçËÆæÂ§á„ÄÇ',

    // ËµÑÊ∫êÂå∫Âùó
    'resources.heading': 'ËµÑÊ∫ê',
    'resources.subtitle': 'ÂºÄÂßã‰ΩøÁî® Gemma 3n ÁöÑÈáçË¶ÅÈìæÊé•„ÄÇ',
    'resources.category.download': '‰∏ãËΩΩÊ®°Âûã',
    'resources.category.api': 'ÂÆòÊñπAPI‰∏éÊåáÂçó',
    'resources.download.hf.title': 'Hugging Face',
    'resources.download.hf.desc': 'Ëé∑ÂèñÂÖ®ÈÉ® Gemma 3n Ê®°Âûã„ÄÅGGUF ÁâàÊú¨ÂèäÁ§æÂå∫ÂæÆË∞ÉÊ®°Âûã„ÄÇ',
    'resources.download.ollama.title': 'Ollama',
    'resources.download.ollama.desc': '‰∏ÄË°åÂëΩ‰ª§Êú¨Âú∞ÊãâÂèñÂπ∂ËøêË°å Gemma 3n Ê®°Âûã„ÄÇ',
    'resources.download.lmstudio.title': 'LM Studio',
    'resources.download.lmstudio.desc': 'Ê°åÈù¢Á´ØÂèØËßÜÂåñÂ∫îÁî®ÔºåËΩªÊùæ‰ΩìÈ™åÂíåËøêË°å Gemma 3n„ÄÇ',
    'resources.api.studio.title': 'Google AI Studio',
    'resources.api.studio.desc': 'Êó†ÈúÄÂÆâË£ÖÔºåÁõ¥Êé•Âú®ÊµèËßàÂô®‰ΩìÈ™å Gemma 3n„ÄÇ',
    'resources.api.keras.title': 'Keras',
    'resources.api.keras.desc': 'Áî® Keras È´òÊïàÊûÑÂª∫ÂíåËÆ≠ÁªÉ Gemma 3n Ê®°Âûã„ÄÇ',
    'resources.api.paper.title': 'ÊäÄÊúØÊä•Âëä',
    'resources.api.paper.desc': 'ÈòÖËØª Google ÂÆòÊñπËÆ∫ÊñáÔºåÊ∑±ÂÖ•‰∫ÜËß£ Gemma 3n Êû∂ÊûÑ„ÄÇ',

    // Demo È°µÈù¢‰∏ªÁªìÊûÑ
    'demo.page.title': 'AI Demo | Interactive Experience',
    'demo.page.description': 'Experience the power of Gemma 3n - run AI models directly in your browser, no installation required.',
    'demo.hero.title': 'Gemma 3n Interactive Experience',
    'demo.hero.subtitle': 'Experience powerful AI features directly in your browser. Code completion ‚Ä¢ Language translation ‚Ä¢ Intelligent Q&A',
    'demo.hero.feature1.title': 'Ultra-fast Response',
    'demo.hero.feature1.desc': 'Millisecond-level AI inference, real-time interaction',
    'demo.hero.feature2.title': 'Privacy First',
    'demo.hero.feature2.desc': 'All data processed locally, never uploaded to the cloud',
    'demo.hero.feature3.title': 'Multi-scenario Support',
    'demo.hero.feature3.desc': 'Coding, translation, chat ‚Äî one model for all',
    'demo.hero.cta': 'Try Now ‚Üí',
    'demo.section.title': 'Interactive AI Demo',
    'demo.section.desc': 'This is a simulated version showing how Gemma 3n works in real-world scenarios. For production, use ONNX.js or WebAssembly to run the real model.',
    'demo.info.title': 'About this Demo',
    'demo.info.current.title': 'Current Features',
    'demo.info.current.feature1': 'Simulates Gemma 3n inference process and response style',
    'demo.info.current.feature2': 'Realistic UI and interaction flow',
    'demo.info.current.feature3': 'Performance metrics based on real hardware data',
    'demo.info.current.feature4': 'Supports three core application scenarios',
    'demo.info.prod.title': 'Production Version',
    'demo.info.prod.feature1': 'Load real Gemma 3n model with ONNX.js',
    'demo.info.prod.feature2': 'Accelerated inference with WebAssembly',
    'demo.info.prod.feature3': 'Full tokenizer and post-processing pipeline',
    'demo.info.prod.feature4': 'Supports model quantization and optimization',
    'demo.tech.title': 'Technical Implementation Path',
    'demo.tech.desc': 'Upgrade the demo to a full-fledged AI application tech stack',
    'demo.tech.frontend.title': 'Frontend Architecture',
    'demo.tech.frontend.engine': 'Lightweight Inference Engine',
    'demo.tech.frontend.engine.code': "// ONNX.js integration\nimport * as ort from 'onnxruntime-web';\n\n// Load model\nconst session = await ort.InferenceSession\n  .create('/models/gemma-3n-e2b.onnx');\n\n// Inference\nconst results = await session.run(feeds);",
    'demo.tech.frontend.wasm': 'WebAssembly Optimization',
    'demo.tech.frontend.wasm.code': "// WebAssembly tokenizer\nimport init, { tokenize } from './pkg/tokenizer.js';\n\n// Initialize WASM module\nawait init();\n\n// High-performance tokenization\nconst tokens = tokenize(inputText);",
    'demo.tech.deploy.title': 'Model Deployment',
    'demo.tech.deploy.convert': 'Model Conversion',
    'demo.tech.deploy.convert1': 'Hugging Face ‚Üí ONNX',
    'demo.tech.deploy.convert2': 'Dynamic quantization (INT8)',
    'demo.tech.deploy.convert3': 'Graph optimization and constant folding',
    'demo.tech.deploy.convert4': 'WebGL backend adaptation',
    'demo.tech.deploy.cdn': 'CDN Distribution',
    'demo.tech.deploy.cdn1': 'Global acceleration with Cloudflare',
    'demo.tech.deploy.cdn2': 'Chunked download strategy',
    'demo.tech.deploy.cdn3': 'Browser cache optimization',
    'demo.tech.deploy.cdn4': 'Progressive loading',
    'demo.tech.deploy.optimize': 'Performance Optimization',
    'demo.tech.deploy.optimize1': 'Web Workers multithreading',
    'demo.tech.deploy.optimize2': 'SharedArrayBuffer',
    'demo.tech.deploy.optimize3': 'WebGPU acceleration (future)',
    'demo.tech.deploy.optimize4': 'Memory pool management',
    'demo.tech.cost.title': 'Zero-cost Solution Advantages',
    'demo.tech.cost.cloud': 'Traditional Cloud AI Cost',
    'demo.tech.cost.cloud1': 'üî¥ OpenAI API: $0.002/1K tokens',
    'demo.tech.cost.cloud2': 'üî¥ Azure OpenAI: $0.0015/1K tokens',
    'demo.tech.cost.cloud3': 'üî¥ Google Cloud AI: $0.001/1K tokens',
    'demo.tech.cost.cloud4': 'üî¥ Monthly: $200-2000 (medium traffic)',
    'demo.tech.cost.device': 'Gemma 3n On-device Solution',
    'demo.tech.cost.device1': '‚úÖ Inference cost: $0',
    'demo.tech.cost.device2': '‚úÖ CDN: $0 (Cloudflare free tier)',
    'demo.tech.cost.device3': '‚úÖ Storage: $0 (static hosting)',
    'demo.tech.cost.device4': '‚úÖ Monthly: $0 + $12/year domain',
    'demo.cta.title': 'Ready to build your AI app?',
    'demo.cta.desc': 'Start with tutorials and master the power of Gemma 3n step by step.',
    'demo.cta.learn': 'Start Learning',
    'demo.cta.toolkit': 'Toolkit',
    'demo.cta.download': 'Download Model',

    // FAQÂå∫Âùó
    'faq.heading': 'Â∏∏ËßÅÈóÆÈ¢ò',
    'faq.subtitle': 'ÊúâÁñëÈóÆÔºüÊàë‰ª¨Êù•Ëß£Á≠î„ÄÇ‰ª•‰∏ãÊòØÂºÄÂèëËÄÖÊúÄÂ∏∏ÈóÆÁöÑ‰∏Ä‰∫õÈóÆÈ¢ò„ÄÇ',
    'faq.q1': 'Gemma 3n ÂèØ‰ª•ÂÖçË¥πÂïÜÁî®ÂêóÔºü',
    'faq.a1': 'ÂèØ‰ª•„ÄÇGemma 3n Ê®°ÂûãÈááÁî®ÂÆΩÊùæËÆ∏ÂèØÂçèËÆÆÔºåÂÖÅËÆ∏ÂÖçË¥πÂïÜÁî®ÂíåÁßëÁ†î‰ΩøÁî®„ÄÇËØ∑Âä°ÂøÖÊü•ÈòÖÂÆòÊñπËÆ∏ÂèØÊù°Ê¨æ„ÄÇ',
    'faq.q2': 'Gemma 3n ÁöÑ"Â§öÊ®°ÊÄÅ"ÂÖ∑‰ΩìÊåá‰ªÄ‰πàÔºü',
    'faq.a2': 'Â§öÊ®°ÊÄÅÊÑèÂë≥ÁùÄÊ®°Âûã‰∏ç‰ªÖËÉΩÁêÜËß£ÊñáÊú¨ÔºåËøòËÉΩÂàÜÊûêÂõæÁâáÂíåÈü≥È¢ëÔºåÈÄÇÁî®‰∫éÊèèËø∞ÁÖßÁâá„ÄÅËØ≠Èü≥ËΩ¨ÂΩïÁ≠âÂ§öÂú∫ÊôØ„ÄÇ',
    'faq.q3': 'Gemma 3n Âíå Gemma 2 Êúâ‰ªÄ‰πàÂå∫Âà´Ôºü',
    'faq.a3': 'Gemma 3n ÈíàÂØπËÆæÂ§áÁ´Ø‰ºòÂåñÔºåÈááÁî® MatFormer Êû∂ÊûÑÔºåÂÜÖÂ≠òÂíåÁÆóÂäõÊïàÁéáÊõ¥È´òÔºåÈùûÂ∏∏ÈÄÇÂêàÊâãÊú∫„ÄÅÁ¨îËÆ∞Êú¨Á≠âÊú¨Âú∞ÈÉ®ÁΩ≤„ÄÇ',
    'faq.q4': 'ÂèØ‰ª•Áî®Ëá™Â∑±ÁöÑÊï∞ÊçÆÂæÆË∞É Gemma 3n ÂêóÔºü',
    'faq.a4': 'ÂΩìÁÑ∂ÂèØ‰ª•„ÄÇGoogle Êèê‰æõ‰∫Ü Keras„ÄÅPyTorch„ÄÅJAX Á≠â‰∏ªÊµÅÊ°ÜÊû∂ÁöÑÂæÆË∞ÉÊñπÊ°àÂíåÂ∑•ÂÖ∑„ÄÇ',
    'faq.q5': 'Â¶Ç‰ΩïÁî® Ollama ËøêË°å Gemma 3nÔºü',
    'faq.a5': 'ÂÆâË£Ö Ollama ÂêéÔºåËøêË°å "ollama run gemma-3n:e4b" Êàñ "ollama run gemma-3n:e2b" Âç≥ÂèØËá™Âä®‰∏ãËΩΩÂπ∂ËøêË°åÂØπÂ∫îÊ®°Âûã„ÄÇ',
    'faq.q6': 'E2B Âíå E4B Êúâ‰ªÄ‰πàÂå∫Âà´Ôºü',
    'faq.a6': 'E2BÔºà2BÂèÇÊï∞ÔºâÊõ¥Â∞èÊõ¥Âø´ÔºåÈÄÇÂêàÁßªÂä®Á´ØÂíåÂø´ÈÄüÊé®ÁêÜÔºõE4BÔºà4BÂèÇÊï∞ÔºâÁ≤æÂ∫¶Êõ¥È´ò‰ΩÜËµÑÊ∫êÈúÄÊ±ÇÊõ¥Â§ß„ÄÇ‰∏§ËÄÖÂùáÈááÁî® MatFormer Êû∂ÊûÑ„ÄÇ',
    'faq.q7': 'ÂèØ‰ª•Âú® Hugging Face ‰∏äÁî® Gemma 3n ÂêóÔºü',
    'faq.a7': 'ÂèØ‰ª•ÔºåÊâÄÊúâ Gemma 3n Ê®°ÂûãÂùáÂ∑≤‰∏äÊû∂ Hugging Face HubÔºåÊîØÊåÅ transformers Â∫ìÁõ¥Êé•Ë∞ÉÁî®„ÄÇ',
    'faq.q8': 'Gemma 3n ÊØî Llama 3 Êõ¥ÈÄÇÂêàÊú¨Âú∞ÈÉ®ÁΩ≤ÂêóÔºü',
    'faq.a8': 'Gemma 3n E4B Âú®Â§öÈ°πÂü∫ÂáÜÊµãËØï‰∏≠‰ºò‰∫é Llama 3 8BÔºå‰∏î‰ΩìÁßØÊõ¥Â∞è„ÄÅÊé®ÁêÜÊõ¥Âø´ÔºåÈùûÂ∏∏ÈÄÇÂêàÊú¨Âú∞ÂíåÈöêÁßÅÂú∫ÊôØ„ÄÇ',
    'faq.q9': 'Gemma 3n ËÉΩÂú® iOS ËÆæÂ§á‰∏äËøêË°åÂêóÔºü',
    'faq.a9': 'ÂèØ‰ª•ÔºåE2B ÁâàÊú¨ÁâπÂà´ÈÄÇÂêàÁßªÂä®Á´Ø„ÄÇÂèØÁî® CoreML ÊàñÈáèÂåñÊ®°ÂûãÂú®ËãπÊûúËÆæÂ§áÈ´òÊïàËøêË°å„ÄÇ',
    'faq.q10': 'Gemma 3n ÂØπÁ°¨‰ª∂Êúâ‰ΩïË¶ÅÊ±ÇÔºü',
    'faq.a10': 'E2B Âè™ÈúÄ 4GB ÂÜÖÂ≠òÂç≥ÂèØËøêË°åÔºåE4B Êé®Ëçê 8GB ‰ª•‰∏ä„ÄÇCPU ‰πüÂèØËøêË°åÔºåËã•Êúâ GPU Êé®ÁêÜÈÄüÂ∫¶Êõ¥Âø´„ÄÇ',

    // newsletter
    'newsletter.heading': 'Ëé∑ÂèñÊúÄÊñ∞ËµÑËÆØ',
    'newsletter.desc': 'ËÆ¢ÈòÖÂêéÂèØÁ¨¨‰∏ÄÊó∂Èó¥Ëé∑Âèñ Gemma 3n ÁöÑÊúÄÊñ∞ÊïôÁ®ã‰∏éÊñ∞Èóª„ÄÇÁªùÊó†ÂûÉÂúæÈÇÆ‰ª∂„ÄÇ',
    'newsletter.placeholder': 'ËØ∑ËæìÂÖ•ÊÇ®ÁöÑÈÇÆÁÆ±',
    'newsletter.subscribe': 'ËÆ¢ÈòÖ',

    // footer
    'footer.rights': '‰øùÁïôÊâÄÊúâÊùÉÂà©„ÄÇ',
    'footer.independent': 'Áã¨Á´ãÈ°πÁõÆÔºå‰∏é Google Êó†ÂÖ≥ËÅî„ÄÇ',

    // Blog
    'blog.title': 'ÂçöÂÆ¢',
    'blog.description': 'Gemma 3n ÂèäÁ´Ø‰æß AI Ê∑±Â∫¶ÊñáÁ´†„ÄÅÊïôÁ®ã‰∏éÂàÜÊûê„ÄÇ',
    'blog.heading': 'ÂçöÂÆ¢‰∏éÊïôÁ®ã',
    'blog.subtitle': 'Ê∑±Â∫¶ÊñáÁ´†„ÄÅÂÆûÁî®ÊåáÂçó‰∏éÂàÜÊûêÔºåÂä©‰Ω†Á≤æÈÄöÁ´Ø‰æß AI„ÄÇ',
    'blog.posted_on': 'ÂèëÂ∏É‰∫é',
  }
} as const;

// Ëé∑ÂèñÁøªËØëÊñáÊú¨ÁöÑÂáΩÊï∞
export function useTranslations(lang: Language) {
  return function t(key: keyof typeof ui[typeof defaultLang]): string {
    return ui[lang]?.[key] || ui[defaultLang][key] || key;
  }
}

// ‰ªéURLË∑ØÂæÑËé∑ÂèñËØ≠Ë®Ä
export function getLangFromUrl(url: URL): Language {
  const [, lang] = url.pathname.split('/');
  if (lang in languages) return lang as Language;
  return defaultLang;
}

// Ëé∑ÂèñÊú¨Âú∞ÂåñË∑ØÂæÑ
export function getLocalizedPath(path: string, lang: Language): string {
  if (lang === defaultLang) {
    return path;
  }
  return `/${lang}${path}`;
}

// Ëé∑ÂèñÊõø‰ª£ËØ≠Ë®ÄË∑ØÂæÑ
export function getAlternateLanguagePaths(currentPath: string, currentLang: Language) {
  const paths: Record<Language, string> = {} as Record<Language, string>;
  
  // ÁßªÈô§ÂΩìÂâçËØ≠Ë®ÄÂâçÁºÄ
  let basePath = currentPath;
  if (currentLang !== defaultLang) {
    basePath = currentPath.replace(`/${currentLang}`, '') || '/';
  }
  
  // ‰∏∫ÊØèÁßçËØ≠Ë®ÄÁîüÊàêË∑ØÂæÑ
  Object.keys(languages).forEach(lang => {
    const language = lang as Language;
    paths[language] = getLocalizedPath(basePath, language);
  });
  
  return paths;
} 